{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02ea6ea566df7354a9a88bcce4c6e714",
     "grade": false,
     "grade_id": "cell-202739b8f3a67536",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "\n",
    "In this project, you will implement the CART (Classification and Regression Tree) algorithm. You will work with the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">Ionosphere Data Set from the UCI Machine Learning Repository</a>, which consists of radar data from a system designed to target free electrons in the ionosphere, and a artificial \"spiral\" dataset. The first dataset will be used to determine if a radar return was \"good\" (i.e. a signal was returned) or \"bad\" (i.e. the signal passed straight through the ionosphere).\n",
    "\n",
    "**You will be using a regression tree with squared loss impurity to do classification. This is possible here because all classification problems can be framed as regression problems. You could also have used a classification tree with Gini impurity equivalently.**\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf891c1be4dc1ae06191273f05e0d68e",
     "grade": false,
     "grade_id": "cell-727395f9bc8b5c9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Implementing Regression Trees</h2>\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need. In addition, you will load two binary classification dataset - the spiral dataset and the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a> dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ad00e0439033b41698216da17731123",
     "grade": false,
     "grade_id": "cell-5445d0afdbc4c054",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.6.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a36ccf5bfced96b28d8966f36446473",
     "grade": false,
     "grade_id": "cell-c219552fd154672d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The code below generates spiral data using the trigonometric functions sine and cosine, then splits the data into train and test segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9495e5cadae1976dadd4b1ad0e63367c",
     "grade": false,
     "grade_id": "cell-9a38f0686e9d323f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points in spiral dataset: 150\n",
      "Number of testing points in spiral dataset: 150\n",
      "Number of features in spiral dataset: 2\n"
     ]
    }
   ],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N) # generate a vector of \"radius\" values\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T # generate a curve that draws circles with increasing radius\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    # Now sample alternating values to generate the test and train sets.\n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr, yTr, xTe, yTe\n",
    "\n",
    "xTrSpiral, yTrSpiral, xTeSpiral, yTeSpiral = spiraldata(150)\n",
    "print(f'Number of training points in spiral dataset: {xTrSpiral.shape[0]}')\n",
    "print(f'Number of testing points in spiral dataset: {xTeSpiral.shape[0]}')\n",
    "print(f'Number of features in spiral dataset: {xTrSpiral.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "66d9205e7c90b804b83fed62414fed1e",
     "grade": false,
     "grade_id": "cell-65f296cbe2f8b1ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Each data point $[\\mathbf{x}]_i$ in the spiral data has 2 dimensions and the label $y_i$ is either $-1$ or $+1$. We can plot `xTrSpiral` to see the points, colored by the label they are associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f986122b0f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(xTrSpiral[:, 0], xTrSpiral[:, 1], s=30, c=yTrSpiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3183e35d16b85a5f474e39a15cd14f5e",
     "grade": false,
     "grade_id": "cell-d73f1d288cf74deb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following code loads the ION dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8223c9331576477c95d446780580ff1",
     "grade": false,
     "grade_id": "cell-134b99a8d0c69131",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points in ION dataset: 281\n",
      "Number of testing points in ION dataset: 70\n",
      "Number of features in ION dataset: 34\n",
      "Training set: (n x d matrix)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$y$</th>\n",
       "      <th>$[\\mathbf{x}]_{1}$</th>\n",
       "      <th>$[\\mathbf{x}]_{2}$</th>\n",
       "      <th>$[\\mathbf{x}]_{3}$</th>\n",
       "      <th>$[\\mathbf{x}]_{4}$</th>\n",
       "      <th>$[\\mathbf{x}]_{5}$</th>\n",
       "      <th>$[\\mathbf{x}]_{6}$</th>\n",
       "      <th>$[\\mathbf{x}]_{7}$</th>\n",
       "      <th>$[\\mathbf{x}]_{8}$</th>\n",
       "      <th>$[\\mathbf{x}]_{9}$</th>\n",
       "      <th>...</th>\n",
       "      <th>$[\\mathbf{x}]_{25}$</th>\n",
       "      <th>$[\\mathbf{x}]_{26}$</th>\n",
       "      <th>$[\\mathbf{x}]_{27}$</th>\n",
       "      <th>$[\\mathbf{x}]_{28}$</th>\n",
       "      <th>$[\\mathbf{x}]_{29}$</th>\n",
       "      <th>$[\\mathbf{x}]_{30}$</th>\n",
       "      <th>$[\\mathbf{x}]_{31}$</th>\n",
       "      <th>$[\\mathbf{x}]_{32}$</th>\n",
       "      <th>$[\\mathbf{x}]_{33}$</th>\n",
       "      <th>$[\\mathbf{x}]_{34}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.83</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>1.91</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.10</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.67</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.81</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-1.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.73</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.23</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>1.85</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.82</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.37</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     $y$  $[\\mathbf{x}]_{1}$  $[\\mathbf{x}]_{2}$  $[\\mathbf{x}]_{3}$  \\\n",
       "0   -1.0                0.55                1.66               -0.56   \n",
       "1   -1.0                0.23               -0.04                1.61   \n",
       "2   -1.0                2.18               -1.05                0.27   \n",
       "3   -1.0               -2.10                1.56                0.55   \n",
       "4   -1.0                1.55                0.18                0.74   \n",
       "..   ...                 ...                 ...                 ...   \n",
       "276  1.0                3.37               -1.57               -1.80   \n",
       "277  1.0               -0.05                0.03                0.39   \n",
       "278  1.0               -1.34               -0.13               -0.36   \n",
       "279  1.0               -1.24                0.13               -0.09   \n",
       "280  1.0               -2.00               -0.19               -0.29   \n",
       "\n",
       "     $[\\mathbf{x}]_{4}$  $[\\mathbf{x}]_{5}$  $[\\mathbf{x}]_{6}$  \\\n",
       "0                  1.01                1.83               -0.46   \n",
       "1                 -0.09               -0.23               -1.59   \n",
       "2                  2.20                2.15               -2.48   \n",
       "3                 -1.53               -0.92               -0.31   \n",
       "4                  0.78                0.18                0.38   \n",
       "..                  ...                 ...                 ...   \n",
       "276               -2.17                0.13                0.04   \n",
       "277                0.11               -0.05                0.09   \n",
       "278                0.46                1.19                0.07   \n",
       "279               -0.27                0.06               -0.03   \n",
       "280               -0.09               -0.04               -0.03   \n",
       "\n",
       "     $[\\mathbf{x}]_{7}$  $[\\mathbf{x}]_{8}$  $[\\mathbf{x}]_{9}$  ...  \\\n",
       "0                  1.91               -1.48               -1.15  ...   \n",
       "1                  1.35                0.34               -0.03  ...   \n",
       "2                 -0.13                1.76                0.14  ...   \n",
       "3                  2.06               -0.30               -0.15  ...   \n",
       "4                 -0.10               -0.21               -0.63  ...   \n",
       "..                  ...                 ...                 ...  ...   \n",
       "276                0.32                0.10               -0.40  ...   \n",
       "277               -0.10                0.17                0.16  ...   \n",
       "278                0.34               -0.38                0.23  ...   \n",
       "279               -0.32                0.09                0.17  ...   \n",
       "280                0.07                0.08               -0.07  ...   \n",
       "\n",
       "     $[\\mathbf{x}]_{25}$  $[\\mathbf{x}]_{26}$  $[\\mathbf{x}]_{27}$  \\\n",
       "0                  -0.51                -0.46                 0.58   \n",
       "1                   0.36                -0.24                 0.06   \n",
       "2                   0.02                 0.27                 0.08   \n",
       "3                  -0.53                 0.30                -0.74   \n",
       "4                   0.01                 0.19                -0.18   \n",
       "..                   ...                  ...                  ...   \n",
       "276                 0.29                 0.14                 0.06   \n",
       "277                -0.09                -0.06                 0.07   \n",
       "278                -0.24                 0.07                 0.18   \n",
       "279                 0.09                -0.06                -0.02   \n",
       "280                 0.00                -0.03                 0.01   \n",
       "\n",
       "     $[\\mathbf{x}]_{28}$  $[\\mathbf{x}]_{29}$  $[\\mathbf{x}]_{30}$  \\\n",
       "0                   0.30                 0.19                -0.30   \n",
       "1                  -0.21                -0.21                -0.04   \n",
       "2                   0.15                -0.22                 0.09   \n",
       "3                   0.03                -0.14                 0.23   \n",
       "4                  -0.18                 0.02                -0.04   \n",
       "..                   ...                  ...                  ...   \n",
       "276                -0.47                 0.30                 0.07   \n",
       "277                -0.15                -0.04                -0.09   \n",
       "278                 0.06                 0.00                -0.66   \n",
       "279                 0.07                 0.04                -0.01   \n",
       "280                -0.02                -0.01                -0.01   \n",
       "\n",
       "     $[\\mathbf{x}]_{31}$  $[\\mathbf{x}]_{32}$  $[\\mathbf{x}]_{33}$  \\\n",
       "0                  -0.48                -0.45                -0.09   \n",
       "1                   0.00                 0.22                 0.08   \n",
       "2                  -0.34                -0.45                -0.15   \n",
       "3                   0.00                -0.18                -0.02   \n",
       "4                  -0.22                 0.19                 0.05   \n",
       "..                   ...                  ...                  ...   \n",
       "276                -0.54                 0.27                -0.12   \n",
       "277                 0.12                -0.15                -0.09   \n",
       "278                 0.11                 0.19                 0.13   \n",
       "279                -0.03                -0.05                -0.04   \n",
       "280                -0.01                -0.00                 0.03   \n",
       "\n",
       "     $[\\mathbf{x}]_{34}$  \n",
       "0                   -0.0  \n",
       "1                   -0.0  \n",
       "2                   -0.0  \n",
       "3                    0.0  \n",
       "4                   -0.0  \n",
       "..                   ...  \n",
       "276                  0.0  \n",
       "277                 -0.0  \n",
       "278                 -0.0  \n",
       "279                 -0.0  \n",
       "280                  0.0  \n",
       "\n",
       "[281 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in some binary test data (labels are -1, +1)\n",
    "data = loadmat(\"ion.mat\")\n",
    "\n",
    "# Load the training data\n",
    "xTrIon  = data['xTr'].T\n",
    "yTrIon  = data['yTr'].flatten()\n",
    "\n",
    "# Load the test data\n",
    "xTeIon  = data['xTe'].T\n",
    "yTeIon  = data['yTe'].flatten()\n",
    "\n",
    "print(f'Number of training points in ION dataset: {xTrIon.shape[0]}')\n",
    "print(f'Number of testing points in ION dataset: {xTeIon.shape[0]}')\n",
    "print(f'Number of features in ION dataset: {xTrIon.shape[1]}')\n",
    "print('Training set: (n x d matrix)')\n",
    "TrIon_for_display = np.concatenate([yTrIon[:, None], xTrIon], axis=1)\n",
    "TrIon_for_display = TrIon_for_display[TrIon_for_display[:, 0].argsort()]\n",
    "\n",
    "display(pd.DataFrame(data=TrIon_for_display,\n",
    "                     columns=['$y$'] + [f'$[\\mathbf{{x}}]_{ {i+1} }$' for i in range(xTrIon.shape[1])]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1471471f16e7153e9f7916d8893fab91",
     "grade": false,
     "grade_id": "cell-c632b2561da0c25c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part One: Implement `sqimpurity` [Graded]\n",
    "\n",
    "First, implement the function **`sqimpurity`**, which takes as input a vector $y$ of $n$ labels and outputs the corresponding squared loss impurity:\n",
    "$$\n",
    "\\sum_{i = 1}^{n} \\left( y_i - \\overline{y} \\right)^2 \\textrm{, where } \\overline{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}.\n",
    "$$\n",
    "\n",
    "Again, the squared loss impurity works fine even though our final objective is classification. This is because the labels are binary and classification problems can be framed as regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "def430a6952c11c5e4cf8b3d9516fe81",
     "grade": false,
     "grade_id": "cell-ec2301f1325f79b5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqimpurity(yTr):\n",
    "    \"\"\"\n",
    "    Computes the squared loss impurity (variance) of the labels.\n",
    "    \n",
    "    Input:\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        squared loss impurity: weighted variance/squared loss impurity of the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    N, = yTr.shape\n",
    "    assert N > 0 # must have at least one sample\n",
    "    impurity = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    impurity=np.sum(np.square(yTr-np.mean(yTr)))\n",
    "    # raise NotImplementedError()\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28e61f2dc5aeb81315a4e40b9306e92d",
     "grade": false,
     "grade_id": "cell-ba20a92528e16fbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: sqimpurity_test1 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test2 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test3 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test4 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test5 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "def sqimpurity_test1():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isscalar(impurity)  # impurity should be scalar\n",
    "\n",
    "def sqimpurity_test2():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return impurity >= 0 # impurity should be nonnegative\n",
    "\n",
    "def sqimpurity_test3():\n",
    "    yTr = np.ones(100) # generate an all one vector as labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isclose(impurity, 0) # impurity should be zero since the labels are homogeneous\n",
    "\n",
    "def sqimpurity_test4():\n",
    "    yTr = np.arange(-5, 6) # generate a vector with mean zero\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    sum_of_squares = np.sum(yTr ** 2) \n",
    "    return np.isclose(impurity, sum_of_squares) # with mean zero, then the impurity should be the sum of squares\n",
    "\n",
    "def sqimpurity_test5():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr)\n",
    "    impurity_grader = sqimpurity_grader(yTr)\n",
    "    return np.isclose(impurity, impurity_grader)\n",
    "\n",
    "runtest(sqimpurity_test1, 'sqimpurity_test1')\n",
    "runtest(sqimpurity_test2, 'sqimpurity_test2')\n",
    "runtest(sqimpurity_test3, 'sqimpurity_test3')\n",
    "runtest(sqimpurity_test4, 'sqimpurity_test4')\n",
    "runtest(sqimpurity_test5, 'sqimpurity_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e4b6cdeffb9e6abb0e20f1426d5e335",
     "grade": true,
     "grade_id": "cell-84a2790fead7b76f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "295c7921f17373bcee865218851434f0",
     "grade": true,
     "grade_id": "cell-835351931df18818",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "15be06d818d669ee3d72cd31f9849813",
     "grade": true,
     "grade_id": "cell-3c3b7a31a818b505",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "465b390daf35d5c677b377e4a98633ee",
     "grade": true,
     "grade_id": "cell-1c7dc9e1879e6a32",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0906343847ad8e1e8800b1c85bd474a7",
     "grade": true,
     "grade_id": "cell-330bca5d42fef37a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "110d11c6a9b86f38e65eb9d517778dac",
     "grade": false,
     "grade_id": "cell-c8238fba4c3de7ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's plot the shape of the impurity function. We vary the mixture of labels in a set of $n$ labels and calculate the impurity of the labels. When the labels are mostly the same, the impurity should be low. When the labels are evenly split between $+1$ and $-1$, the impurity should be the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraction_pos</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.2</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fraction_pos  impurity\n",
       "0            1.0       0.0\n",
       "1            0.9       3.6\n",
       "2            0.8       6.4\n",
       "3            0.7       8.4\n",
       "4            0.6       9.6\n",
       "5            0.5      10.0\n",
       "6            0.4       9.6\n",
       "7            0.3       8.4\n",
       "8            0.2       6.4\n",
       "9            0.1       3.6\n",
       "10           0.0       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Squared loss impurity of labels')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 10\n",
    "y = np.ones(size)\n",
    "fraction_pos, impurities = [], []\n",
    "for i in range(size):\n",
    "    fraction_pos.append(sum([y == 1]) / size)\n",
    "    impurities.append(sqimpurity(y))\n",
    "    y[i] = -1\n",
    "fraction_pos.append(sum([y == 1]) / size)\n",
    "impurities.append(sqimpurity(y))\n",
    "\n",
    "display(pd.DataFrame(data={'fraction_pos': fraction_pos, 'impurity': impurities}))\n",
    "plt.plot(fraction_pos, impurities)\n",
    "plt.grid()\n",
    "plt.xlabel('Fraction of positive labels (1 - fraction of negative labels)')\n",
    "plt.ylabel('Squared loss impurity of labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "27c84f41b6f8c383bd75fccc138ad42c",
     "grade": false,
     "grade_id": "cell-4730d6e94ace8e06",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two: Implement `sqsplit` [Graded]\n",
    "\n",
    "Now implement **`sqsplit`**, which takes as input a data set of size $n \\times d$ with labels and computes the best feature and the threshold/cut of the optimal split based on the squared loss impurity. The function outputs a feature dimension `0 <= feature < d`, a cut threshold `cut`, and the impurity loss `bestloss` of this best split.\n",
    "\n",
    "Recall in the CART algorithm that, to find the split with the minimum impurity, you iterate over all features and cut values along each feature. We enforce that the cut value be the average of the two consecutive data points' feature values.\n",
    "\n",
    "You should calculate the impurity of a node of data $S$ with two branches $S_L$ and $S_R$ as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(S) &= \\frac{\\left| S_L \\right|}{|S|} I \\left( S_L \\right) + \\frac{\\left| S_R \\right|}{|S|} I \\left( S_R \\right)\\\\\n",
    "&= \\frac{1}{|S|}\\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\frac{1}{|S|} \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\\\\\n",
    "&\\propto \\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Implementation Notes:**\n",
    "- For calculating the impurity of a node, you should just return the sum of left and right impurities instead of the average.\n",
    "- Returned `feature` must be 0-indexed as is consistent with programming in Python.\n",
    "- If along a feature $f$, two data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ have the same value, avoid splitting between them; move to the next pair of data points.\n",
    "\n",
    "For example, with the following `xTr` of size $4 \\times 3$ and `yTr` for 4 points:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2\\\\\n",
    "2 & 0 & 1\\\\\n",
    "0 & 0 & 1\\\\\n",
    "2 & 1 & 2\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1\\\\1\\\\1\\\\-1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "among possible features `[0, 1, 2]`, the best split would be at `feature = 1` and `cut = (0 + 1) / 2 = 0.5`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "If you're stuck, we recommend that you start with the naïve algorithm for finding the best split, which involves a double loop over all features `0 <= f < d` and all cut values `xTr[0, f] < (xTr[i, f] + xTr[i+1, f]) / 2 < xTr[n-1, f]` (with `xTr` sorted along feature `f`). This algorithm thus calculates impurities for `d(n-1)` splits. Here's the pseudocode:\n",
    "\n",
    "<center><img src=\"cart-id3_best_split_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d9033ab9fb3dbbcb866dcc1bd45db8f4",
     "grade": false,
     "grade_id": "cell-sqsplit",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqsplit(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Finds the best feature, cut value, and impurity for a split of (xTr, yTr) based on squared loss impurity.\n",
    "    \n",
    "    Input:\n",
    "        xTr: n x d matrix of data points\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature (keep in mind this is 0-indexed)\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: squared loss impurity of the best cut\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    assert d > 0 # must have at least one dimension\n",
    "    assert n > 1 # must have at least two samples\n",
    "    \n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    for col in range(d):\n",
    "        \n",
    "        sortIdx = xTr[:,col].argsort()      # sort data along that dimensions\n",
    "        xsorted = xTr[sortIdx,col]          # sorted feature values\n",
    "        ysorted = yTr[sortIdx]              # sorted labels\n",
    "    \n",
    "        \n",
    "        for row in range(n-1):              # n-1 so that we don't get an 'index out of range' error\n",
    "            \n",
    "            # check where values change\n",
    "                \n",
    "            if xsorted[row + 1] > xsorted[row]:\n",
    "                Sl = ysorted[:row+1]\n",
    "                Sr = ysorted[row+1:]\n",
    "                \n",
    "                loss=sqimpurity(Sl)+sqimpurity(Sr)\n",
    "            \n",
    "                if loss < bestloss:\n",
    "                    bestloss=loss\n",
    "                    feature=col\n",
    "                    cut =(xsorted[row]+xsorted[row + 1])/2\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return feature, cut, bestloss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edbc3579da8f3891cc0b907caf6f397f",
     "grade": false,
     "grade_id": "cell-sqsplit_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.58 seconds\n",
      "The best split is on feature 2 on value 0.304\n",
      "Your tree split on feature 2 on value: 0.304 \n",
      "\n",
      "Running Test: sqsplit_test1 ... ✔ Passed!\n",
      "Running Test: sqsplit_test2 ... ✔ Passed!\n",
      "Running Test: sqsplit_test3 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# The tests below check that your sqsplit function returns the correct values for several different input datasets\n",
    "\n",
    "t0 = time.time()\n",
    "fid, cut, loss = sqsplit(xTrIon,yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {:0.2f} seconds'.format(t1-t0))\n",
    "print(\"The best split is on feature 2 on value 0.304\")\n",
    "print(\"Your tree split on feature %i on value: %2.3f \\n\" % (fid,cut))\n",
    "\n",
    "def sqsplit_test1():\n",
    "    a = np.isclose(sqsplit(xor4, yor4)[2] / len(yor4), .25)\n",
    "    b = np.isclose(sqsplit(xor3, yor3)[2] / len(yor3), .25)\n",
    "    c = np.isclose(sqsplit(xor2, yor2)[2] / len(yor2), .25)\n",
    "    return a and b and c\n",
    "\n",
    "def sqsplit_test2():\n",
    "    x = np.array(range(1000)).reshape(-1,1)\n",
    "    y = np.hstack([np.ones(500),-1*np.ones(500)]).T\n",
    "    _, cut, _ = sqsplit(x, y)\n",
    "    return cut <= 500 or cut >= 499\n",
    "\n",
    "def sqsplit_test3():\n",
    "    fid, cut, loss = sqsplit(xor5,yor5)\n",
    "    # cut should be 0.5 but 0 is also accepted\n",
    "    return fid == 0 and (cut >= 0 or cut <= 1) and np.isclose(loss / len(yor5), 2/3)\n",
    "\n",
    "runtest(sqsplit_test1,'sqsplit_test1')\n",
    "runtest(sqsplit_test2,'sqsplit_test2')\n",
    "runtest(sqsplit_test3,'sqsplit_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cbffe6477a05dd8d9db574f0d94edcfa",
     "grade": true,
     "grade_id": "cell-sqsplit_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0b0da843388d7a9ddf59c80d408d3859",
     "grade": true,
     "grade_id": "cell-sqsplit_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86d84abb3781b6b263676a20c5ed6418",
     "grade": true,
     "grade_id": "cell-sqsplit_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d358dfc0f25657b18024284428aec959",
     "grade": false,
     "grade_id": "cell-985b43cf6a0b1e16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Three: Implement `cart` [Graded]\n",
    "\n",
    "In this section, you will implement the function **`cart`**, which returns a regression tree based on the minimum squared loss splitting rule. You should use the function `sqsplit` to make your splits.\n",
    "\n",
    "**Implementation Notes:**\n",
    "We've provided a tree structure in the form of `TreeNode` for you that can be used for both leaves and nodes. To represent the leaves, you would set all fields except `prediction` to `None`.\n",
    "\n",
    "Non-leaf nodes will have non-`None` fields for all except `prediction`:\n",
    "1. `left`: node describing left subtree\n",
    "2. `right`: node describing right subtree\n",
    "3. `feature`: index of feature to cut (0-indexed as returned by `sqsplit`)\n",
    "4. `cut`: cutoff value $t$ ($\\leq t$: left and $> t$: right)\n",
    "5. `prediction`: `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0903afbf0f71752d3ae3caa2a24e8360",
     "grade": false,
     "grade_id": "cell-919899ceb491184f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"\n",
    "    Tree class.\n",
    "    \n",
    "    (You don't _need_ to add any methods or fields here but feel\n",
    "    free to if you like. The tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, feature, cut, prediction):\n",
    "        # Check that all or no arguments are None\n",
    "        node_or_leaf_args = [left, right, feature, cut]\n",
    "        assert all([arg == None for arg in node_or_leaf_args]) or all([arg != None for arg in node_or_leaf_args])\n",
    "        \n",
    "        # Check that all None <==> leaf <==> prediction not None\n",
    "        # Check that all non-None <==> non-leaf <==> prediction is None\n",
    "        if all([arg == None for arg in node_or_leaf_args]):\n",
    "            assert prediction is not None\n",
    "        if all([arg != None for arg in node_or_leaf_args]):\n",
    "            assert prediction is None\n",
    "        \n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.feature = feature \n",
    "        self.cut = cut\n",
    "        self.prediction = prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7eac0b2166f8fddde8c51b4ab8470e47",
     "grade": false,
     "grade_id": "cell-5b554dfec9394ba9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell contains some examples of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a tree that predicts everything as zero ==> prediction 0\n",
    "# In this case, it has no left or right children (it is a leaf node) ==> left = None, right = None, feature = None, cut = None\n",
    "root = TreeNode(None, None, None, None, 0)\n",
    "\n",
    "\n",
    "# The following that a tree with depth 2 or a tree with one split \n",
    "\n",
    "# The tree will return a prediction of 1 if an example falls under the left subtree\n",
    "# Otherwise it will return a prediction of 2\n",
    "# To start, first create two leaf node\n",
    "left_leaf = TreeNode(None, None, None, None, 1)\n",
    "right_leaf = TreeNode(None, None, None, None, 2)\n",
    "\n",
    "# Now create the parent or the root\n",
    "# Suppose we split at feature 0 and cut of 1 and the prediction is None\n",
    "root2 = TreeNode(left_leaf, right_leaf, 0, 1, None)\n",
    "\n",
    "# Now root2 is the tree we desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9c2c5f1f300c14fa009977cd29781d7c",
     "grade": false,
     "grade_id": "cell-d6c63e37fd6c395f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the function `cart` using **recursion** (you call `cart` on the left and right subtrees inside the `cart` function). Recall the pseudocode for the CART algorithm.\n",
    "\n",
    "**NOTE:** In this implementation, you will be using **`np.mean`** for `prediction` argument. To check that floating point values in `xTr` are the same or not, you can use `np.isclose(xTr, xTr[0])`, which returns a list of `True` and `False` based on how different the rows of `xTr` are from the vector `xTr[0]`.\n",
    "\n",
    "<center><img src=\"cart-id3_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de3750e532687607733d83dcc9a0a2dc",
     "grade": false,
     "grade_id": "cell-cart",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cart(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Builds a CART tree.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data\n",
    "        yTr:      n-dimensional vector\n",
    "\n",
    "    Output:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    node = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc022deb75c9f2be5071b44c001e6d82",
     "grade": false,
     "grade_id": "cell-cart_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: cart_test1 ... ✖ Failed! Your code raises an exception. The following is the traceback of the failure:\n",
      "  File \"/home/codio/workspace/helper.py\", line 29, in runtest\n",
      "    if test():\n",
      "   File \"<ipython-input-21-a19ac5689e6b>\", line 5, in cart_test1\n",
      "    t=cart(xor4,yor4)\n",
      "   File \"<ipython-input-20-9fd3766731aa>\", line 16, in cart\n",
      "    raise NotImplementedError()\n",
      "\n",
      "Running Test: cart_test2 ... ✖ Failed! Your code raises an exception. The following is the traceback of the failure:\n",
      "  File \"/home/codio/workspace/helper.py\", line 29, in runtest\n",
      "    if test():\n",
      "   File \"<ipython-input-21-a19ac5689e6b>\", line 11, in cart_test2\n",
      "    t = cart(xor4,y);\n",
      "   File \"<ipython-input-20-9fd3766731aa>\", line 16, in cart\n",
      "    raise NotImplementedError()\n",
      "\n",
      "Running Test: cart_test3 ... ✖ Failed! Your code raises an exception. The following is the traceback of the failure:\n",
      "  File \"/home/codio/workspace/helper.py\", line 29, in runtest\n",
      "    if test():\n",
      "   File \"<ipython-input-21-a19ac5689e6b>\", line 22, in cart_test3\n",
      "    t = cart(xRep, yRep)\n",
      "   File \"<ipython-input-20-9fd3766731aa>\", line 16, in cart\n",
      "    raise NotImplementedError()\n",
      "\n",
      "Running Test: cart_test4 ... ✖ Failed! Your code raises an exception. The following is the traceback of the failure:\n",
      "  File \"/home/codio/workspace/helper.py\", line 29, in runtest\n",
      "    if test():\n",
      "   File \"<ipython-input-21-a19ac5689e6b>\", line 32, in cart_test4\n",
      "    t = cart(X, y)\n",
      "   File \"<ipython-input-20-9fd3766731aa>\", line 16, in cart\n",
      "    raise NotImplementedError()\n",
      "\n",
      "Running Test: cart_test5 ... ✖ Failed! Your code raises an exception. The following is the traceback of the failure:\n",
      "  File \"/home/codio/workspace/helper.py\", line 29, in runtest\n",
      "    if test():\n",
      "   File \"<ipython-input-21-a19ac5689e6b>\", line 49, in cart_test5\n",
      "    t = cart(X, y) # your cart algorithm should generate one split\n",
      "   File \"<ipython-input-20-9fd3766731aa>\", line 16, in cart\n",
      "    raise NotImplementedError()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The tests below check that your implementation of cart  returns the correct predicted values for a sample dataset\n",
    "\n",
    "#test case 1\n",
    "def cart_test1():\n",
    "    t=cart(xor4,yor4)\n",
    "    return DFSxor(t)\n",
    "\n",
    "#test case 2\n",
    "def cart_test2():\n",
    "    y = np.random.rand(16);\n",
    "    t = cart(xor4,y);\n",
    "    yTe = DFSpreds(t)[:];\n",
    "    # Check that every label appears exactly once in the tree\n",
    "    y.sort()\n",
    "    yTe.sort()\n",
    "    return np.all(np.isclose(y, yTe))\n",
    "\n",
    "#test case 3\n",
    "def cart_test3():\n",
    "    xRep = np.concatenate([xor2, xor2])\n",
    "    yRep = np.concatenate([yor2, 1-yor2])\n",
    "    t = cart(xRep, yRep)\n",
    "    return DFSxorUnsplittable(t)\n",
    "\n",
    "#test case 4\n",
    "def cart_test4():\n",
    "    X = np.ones((5, 2)) # Create a dataset with identical examples\n",
    "    y = np.ones(5)\n",
    "    \n",
    "    # On this dataset, your cart algorithm should return a single leaf\n",
    "    # node with prediction equal to 1\n",
    "    t = cart(X, y)\n",
    "    \n",
    "    # t has no children\n",
    "    children_check = (t.left is None) and (t.right is None) \n",
    "    \n",
    "    # Make sure t does not cut any feature and at any value\n",
    "    feature_check = (t.feature is None) and (t.cut is None)\n",
    "    \n",
    "    # Check t's prediction\n",
    "    prediction_check = np.isclose(t.prediction, 1)\n",
    "    return children_check and feature_check and prediction_check\n",
    "\n",
    "#test case 5\n",
    "def cart_test5():\n",
    "    X = np.arange(4).reshape(-1, 1)\n",
    "    y = np.array([0, 0, 1, 1])\n",
    "\n",
    "    t = cart(X, y) # your cart algorithm should generate one split\n",
    "    \n",
    "    # check whether you set t.feature and t.cut to something\n",
    "    return t.feature is not None and t.cut is not None\n",
    "\n",
    "\n",
    "\n",
    "runtest(cart_test1,'cart_test1')\n",
    "runtest(cart_test2,'cart_test2')\n",
    "runtest(cart_test3,'cart_test3')\n",
    "runtest(cart_test4,'cart_test4')\n",
    "runtest(cart_test5,'cart_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b47e3031bb2beb61cae1c347d53b283",
     "grade": true,
     "grade_id": "cell-cart_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e322a7a1bab0a83d65ad27517c275cab",
     "grade": true,
     "grade_id": "cell-cart_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d6bf0051fdf9a7296e893eb66c16b96",
     "grade": true,
     "grade_id": "cell-cart_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07b92c1bbd6474aa04712ddb4dbcc0ca",
     "grade": true,
     "grade_id": "cell-cart_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e3a55c57681ae193e2771512f553e528",
     "grade": true,
     "grade_id": "cell-cart_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "faf306bddae40ade7985dcf6439b861c",
     "grade": false,
     "grade_id": "cell-91bac7e5c5968bbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Four: Implement `evaltree` [Graded]\n",
    "\n",
    "Implement the function **`evaltree`**, which evaluates a decision tree on a given test data set. You essentially need to traverse the tree until you end up in a leaf, where you return the `prediction` value of the leaf. Like the `cart` function, you can call `evaltree` on the left subtree and right subtree on testing points that fall in the corresponding subtrees.\n",
    "\n",
    "Here's some inspiration:\n",
    "1. If the `tree` is a leaf, i.e. the left and right subtrees are `None`, return `tree.prediction` for all `m` testing points.\n",
    "2. If the `tree` is non-leaf, using `tree.feature` and `tree.cut` find testing points with the feature value less than/equal to the threshold and greater than. Now, you can call `evaltree` on `tree.left` and the left set of testing points to obtain the left set's predictions. Then obtain the predictions for the right set, and return all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54f3f523c3529265fb0ce24f6fea9361",
     "grade": false,
     "grade_id": "cell-evaltree",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evaltree(tree, xTe):\n",
    "    \"\"\"\n",
    "    Evaluates testing points in xTe using decision tree root.\n",
    "    \n",
    "    Input:\n",
    "        tree: TreeNode decision tree\n",
    "        xTe:  m x d matrix of data points\n",
    "    \n",
    "    Output:\n",
    "        pred: m-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m, d = xTe.shape\n",
    "    preds = np.zeros(m)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59551843c4d5ab8b8c0924087bb9202c",
     "grade": false,
     "grade_id": "cell-evaltree_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7d5e4a75b8bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrIon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrIon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-9fd3766731aa>\u001b[0m in \u001b[0;36mcart\u001b[0;34m(xTr, yTr)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The following tests check that your implementation of evaltree returns the correct predictions for two sample trees\n",
    "\n",
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)\n",
    "\n",
    "#test case 1\n",
    "def evaltree_test1():\n",
    "    t = cart(xor4,yor4)\n",
    "    xor4te = xor4 + (np.sign(xor4 - .5) * .1)\n",
    "    inds = np.arange(16)\n",
    "    np.random.shuffle(inds)\n",
    "    # Check that shuffling and expanding the data doesn't affect the predictions\n",
    "    return np.all(np.isclose(evaltree(t, xor4te[inds,:]), yor4[inds]))\n",
    "\n",
    "#test case 2\n",
    "def evaltree_test2():\n",
    "    a = TreeNode(None, None, None, None, 1)\n",
    "    b = TreeNode(None, None, None, None, -1)\n",
    "    c = TreeNode(None, None, None, None, 0)\n",
    "    d = TreeNode(None, None, None, None, -1)\n",
    "    e = TreeNode(None, None, None, None, -1)\n",
    "    x = TreeNode(a, b, 0, 10, None)\n",
    "    y = TreeNode(x, c, 0, 20, None)\n",
    "    z = TreeNode(d, e, 0, 40, None)\n",
    "    t = TreeNode(y, z, 0, 30, None)\n",
    "    # Check that the custom tree evaluates correctly\n",
    "    return np.all(np.isclose(\n",
    "            evaltree(t, np.array([[45, 35, 25, 15, 5]]).T),\n",
    "            np.array([-1, -1, 0, -1, 1])))\n",
    "\n",
    "runtest(evaltree_test1,'evaltree_test1')\n",
    "runtest(evaltree_test2,'evaltree_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d772deb41878ca9f83b8c3661ac75b67",
     "grade": true,
     "grade_id": "cell-evaltree_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "68d89de2748bcdc384eb209588ffb84d",
     "grade": true,
     "grade_id": "cell-evaltree_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dfac59f41d855c0f8b4e1e7bf394c7d",
     "grade": false,
     "grade_id": "cell-030c38de62d6adc6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Training and Testing a Classification Tree on the ION Dataset</h3>\n",
    "\n",
    "<p> The following code create a classification tree on the ION dataset and then apply the learned tree to an unknown dataset. If you implement everything correctly, you should get a training RSME that is close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "efc7b56e0189a0d123d3ac40b1011afa",
     "grade": false,
     "grade_id": "cell-534ce4bb724b2f26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Visualize Your Tree</h3>\n",
    "\n",
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the ion data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "694a973d8d7c2709f769a08c64cf8f0f",
     "grade": false,
     "grade_id": "cell-8059df3cc7cdb39d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr,w=None,b=0):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "\n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    \n",
    "    if w is not None:\n",
    "        w = np.array(w).flatten()\n",
    "        alpha = -1 * b / (w ** 2).sum()\n",
    "        plt.quiver(w[0] * alpha, w[1] * alpha,\n",
    "            w[0], w[1], linewidth=2, color=[0,1,0])\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "\n",
    "tree=cart(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X:evaltree(tree,X), xTrSpiral, yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evaltree(tree,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evaltree(tree,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f1fe9d13867759f9403c562fe900853a",
     "grade": false,
     "grade_id": "cell-8ad51604f6c3e28c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def onclick_cart(event):\n",
    "    \"\"\"\n",
    "    Visualize cart, including new point\n",
    "    \"\"\"\n",
    "    global xTraining,labels\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTraining = np.concatenate((xTraining,pos), axis = 0)\n",
    "    labels.append(label);\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(labels)\n",
    "        \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get decision tree\n",
    "    tree=cart(xTraining,np.array(labels).flatten())\n",
    "    fun = lambda X:evaltree(tree,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTraining[labels == c,0],\n",
    "            xTraining[labels == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "\n",
    "%matplotlib notebook\n",
    "xTraining= np.array([[5,6]])\n",
    "labels = [1]\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_cart)\n",
    "plt.title('Click to add positive points and use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "\n",
    "Scikit-learn also provides an implementation of [Regression Trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) (and [Decision Tree Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). The usage is pretty straight-forward: define the regression tree with the impurity function (and other settings), fit to the training set, and evaluate on any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "t0 = time.time()\n",
    "tree = DecisionTreeRegressor(\n",
    "    criterion='mse', # Impurity function = Mean Squared Error (squared loss)\n",
    "    splitter='best', # Take the best split\n",
    "    max_depth=None, # Expand the tree to the maximum depth possible\n",
    ")\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((tree.predict(xTrSpiral) - yTrSpiral)**2)\n",
    "te_err   = np.mean((tree.predict(xTeSpiral) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also provides a tree plotting function, which is again quite simple to use. This is extremely useful while debugging a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "_ = plot_tree(tree, ax=ax, precision=2, feature_names=[f'$[\\mathbf{{x}}]_{i+1}$' for i in range(2)], filled=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
