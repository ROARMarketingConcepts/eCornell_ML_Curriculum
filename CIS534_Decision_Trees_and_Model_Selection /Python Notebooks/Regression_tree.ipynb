{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02ea6ea566df7354a9a88bcce4c6e714",
     "grade": false,
     "grade_id": "cell-202739b8f3a67536",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "\n",
    "In this project, you will implement the CART (Classification and Regression Tree) algorithm. You will work with the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">Ionosphere Data Set from the UCI Machine Learning Repository</a>, which consists of radar data from a system designed to target free electrons in the ionosphere, and a artificial \"spiral\" dataset. The first dataset will be used to determine if a radar return was \"good\" (i.e. a signal was returned) or \"bad\" (i.e. the signal passed straight through the ionosphere).\n",
    "\n",
    "**You will be using a regression tree with squared loss impurity to do classification. This is possible here because all classification problems can be framed as regression problems. You could also have used a classification tree with Gini impurity equivalently.**\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf891c1be4dc1ae06191273f05e0d68e",
     "grade": false,
     "grade_id": "cell-727395f9bc8b5c9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Implementing Regression Trees</h2>\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need. In addition, you will load two binary classification dataset - the spiral dataset and the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a> dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ad00e0439033b41698216da17731123",
     "grade": false,
     "grade_id": "cell-5445d0afdbc4c054",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.6.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a36ccf5bfced96b28d8966f36446473",
     "grade": false,
     "grade_id": "cell-c219552fd154672d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The code below generates spiral data using the trigonometric functions sine and cosine, then splits the data into train and test segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9495e5cadae1976dadd4b1ad0e63367c",
     "grade": false,
     "grade_id": "cell-9a38f0686e9d323f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points in spiral dataset: 150\n",
      "Number of testing points in spiral dataset: 150\n",
      "Number of features in spiral dataset: 2\n"
     ]
    }
   ],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N) # generate a vector of \"radius\" values\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T # generate a curve that draws circles with increasing radius\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    # Now sample alternating values to generate the test and train sets.\n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr, yTr, xTe, yTe\n",
    "\n",
    "xTrSpiral, yTrSpiral, xTeSpiral, yTeSpiral = spiraldata(150)\n",
    "print(f'Number of training points in spiral dataset: {xTrSpiral.shape[0]}')\n",
    "print(f'Number of testing points in spiral dataset: {xTeSpiral.shape[0]}')\n",
    "print(f'Number of features in spiral dataset: {xTrSpiral.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "66d9205e7c90b804b83fed62414fed1e",
     "grade": false,
     "grade_id": "cell-65f296cbe2f8b1ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Each data point $[\\mathbf{x}]_i$ in the spiral data has 2 dimensions and the label $y_i$ is either $-1$ or $+1$. We can plot `xTrSpiral` to see the points, colored by the label they are associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f065add9eb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(xTrSpiral[:, 0], xTrSpiral[:, 1], s=30, c=yTrSpiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3183e35d16b85a5f474e39a15cd14f5e",
     "grade": false,
     "grade_id": "cell-d73f1d288cf74deb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following code loads the ION dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8223c9331576477c95d446780580ff1",
     "grade": false,
     "grade_id": "cell-134b99a8d0c69131",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points in ION dataset: 281\n",
      "Number of testing points in ION dataset: 70\n",
      "Number of features in ION dataset: 34\n",
      "Training set: (n x d matrix)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$y$</th>\n",
       "      <th>$[\\mathbf{x}]_{1}$</th>\n",
       "      <th>$[\\mathbf{x}]_{2}$</th>\n",
       "      <th>$[\\mathbf{x}]_{3}$</th>\n",
       "      <th>$[\\mathbf{x}]_{4}$</th>\n",
       "      <th>$[\\mathbf{x}]_{5}$</th>\n",
       "      <th>$[\\mathbf{x}]_{6}$</th>\n",
       "      <th>$[\\mathbf{x}]_{7}$</th>\n",
       "      <th>$[\\mathbf{x}]_{8}$</th>\n",
       "      <th>$[\\mathbf{x}]_{9}$</th>\n",
       "      <th>...</th>\n",
       "      <th>$[\\mathbf{x}]_{25}$</th>\n",
       "      <th>$[\\mathbf{x}]_{26}$</th>\n",
       "      <th>$[\\mathbf{x}]_{27}$</th>\n",
       "      <th>$[\\mathbf{x}]_{28}$</th>\n",
       "      <th>$[\\mathbf{x}]_{29}$</th>\n",
       "      <th>$[\\mathbf{x}]_{30}$</th>\n",
       "      <th>$[\\mathbf{x}]_{31}$</th>\n",
       "      <th>$[\\mathbf{x}]_{32}$</th>\n",
       "      <th>$[\\mathbf{x}]_{33}$</th>\n",
       "      <th>$[\\mathbf{x}]_{34}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.83</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>1.91</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.10</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.67</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.81</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-1.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.73</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.23</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>1.85</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.82</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.37</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     $y$  $[\\mathbf{x}]_{1}$  $[\\mathbf{x}]_{2}$  $[\\mathbf{x}]_{3}$  \\\n",
       "0   -1.0                0.55                1.66               -0.56   \n",
       "1   -1.0                0.23               -0.04                1.61   \n",
       "2   -1.0                2.18               -1.05                0.27   \n",
       "3   -1.0               -2.10                1.56                0.55   \n",
       "4   -1.0                1.55                0.18                0.74   \n",
       "..   ...                 ...                 ...                 ...   \n",
       "276  1.0                3.37               -1.57               -1.80   \n",
       "277  1.0               -0.05                0.03                0.39   \n",
       "278  1.0               -1.34               -0.13               -0.36   \n",
       "279  1.0               -1.24                0.13               -0.09   \n",
       "280  1.0               -2.00               -0.19               -0.29   \n",
       "\n",
       "     $[\\mathbf{x}]_{4}$  $[\\mathbf{x}]_{5}$  $[\\mathbf{x}]_{6}$  \\\n",
       "0                  1.01                1.83               -0.46   \n",
       "1                 -0.09               -0.23               -1.59   \n",
       "2                  2.20                2.15               -2.48   \n",
       "3                 -1.53               -0.92               -0.31   \n",
       "4                  0.78                0.18                0.38   \n",
       "..                  ...                 ...                 ...   \n",
       "276               -2.17                0.13                0.04   \n",
       "277                0.11               -0.05                0.09   \n",
       "278                0.46                1.19                0.07   \n",
       "279               -0.27                0.06               -0.03   \n",
       "280               -0.09               -0.04               -0.03   \n",
       "\n",
       "     $[\\mathbf{x}]_{7}$  $[\\mathbf{x}]_{8}$  $[\\mathbf{x}]_{9}$  ...  \\\n",
       "0                  1.91               -1.48               -1.15  ...   \n",
       "1                  1.35                0.34               -0.03  ...   \n",
       "2                 -0.13                1.76                0.14  ...   \n",
       "3                  2.06               -0.30               -0.15  ...   \n",
       "4                 -0.10               -0.21               -0.63  ...   \n",
       "..                  ...                 ...                 ...  ...   \n",
       "276                0.32                0.10               -0.40  ...   \n",
       "277               -0.10                0.17                0.16  ...   \n",
       "278                0.34               -0.38                0.23  ...   \n",
       "279               -0.32                0.09                0.17  ...   \n",
       "280                0.07                0.08               -0.07  ...   \n",
       "\n",
       "     $[\\mathbf{x}]_{25}$  $[\\mathbf{x}]_{26}$  $[\\mathbf{x}]_{27}$  \\\n",
       "0                  -0.51                -0.46                 0.58   \n",
       "1                   0.36                -0.24                 0.06   \n",
       "2                   0.02                 0.27                 0.08   \n",
       "3                  -0.53                 0.30                -0.74   \n",
       "4                   0.01                 0.19                -0.18   \n",
       "..                   ...                  ...                  ...   \n",
       "276                 0.29                 0.14                 0.06   \n",
       "277                -0.09                -0.06                 0.07   \n",
       "278                -0.24                 0.07                 0.18   \n",
       "279                 0.09                -0.06                -0.02   \n",
       "280                 0.00                -0.03                 0.01   \n",
       "\n",
       "     $[\\mathbf{x}]_{28}$  $[\\mathbf{x}]_{29}$  $[\\mathbf{x}]_{30}$  \\\n",
       "0                   0.30                 0.19                -0.30   \n",
       "1                  -0.21                -0.21                -0.04   \n",
       "2                   0.15                -0.22                 0.09   \n",
       "3                   0.03                -0.14                 0.23   \n",
       "4                  -0.18                 0.02                -0.04   \n",
       "..                   ...                  ...                  ...   \n",
       "276                -0.47                 0.30                 0.07   \n",
       "277                -0.15                -0.04                -0.09   \n",
       "278                 0.06                 0.00                -0.66   \n",
       "279                 0.07                 0.04                -0.01   \n",
       "280                -0.02                -0.01                -0.01   \n",
       "\n",
       "     $[\\mathbf{x}]_{31}$  $[\\mathbf{x}]_{32}$  $[\\mathbf{x}]_{33}$  \\\n",
       "0                  -0.48                -0.45                -0.09   \n",
       "1                   0.00                 0.22                 0.08   \n",
       "2                  -0.34                -0.45                -0.15   \n",
       "3                   0.00                -0.18                -0.02   \n",
       "4                  -0.22                 0.19                 0.05   \n",
       "..                   ...                  ...                  ...   \n",
       "276                -0.54                 0.27                -0.12   \n",
       "277                 0.12                -0.15                -0.09   \n",
       "278                 0.11                 0.19                 0.13   \n",
       "279                -0.03                -0.05                -0.04   \n",
       "280                -0.01                -0.00                 0.03   \n",
       "\n",
       "     $[\\mathbf{x}]_{34}$  \n",
       "0                   -0.0  \n",
       "1                   -0.0  \n",
       "2                   -0.0  \n",
       "3                    0.0  \n",
       "4                   -0.0  \n",
       "..                   ...  \n",
       "276                  0.0  \n",
       "277                 -0.0  \n",
       "278                 -0.0  \n",
       "279                 -0.0  \n",
       "280                  0.0  \n",
       "\n",
       "[281 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in some binary test data (labels are -1, +1)\n",
    "data = loadmat(\"ion.mat\")\n",
    "\n",
    "# Load the training data\n",
    "xTrIon  = data['xTr'].T\n",
    "yTrIon  = data['yTr'].flatten()\n",
    "\n",
    "# Load the test data\n",
    "xTeIon  = data['xTe'].T\n",
    "yTeIon  = data['yTe'].flatten()\n",
    "\n",
    "print(f'Number of training points in ION dataset: {xTrIon.shape[0]}')\n",
    "print(f'Number of testing points in ION dataset: {xTeIon.shape[0]}')\n",
    "print(f'Number of features in ION dataset: {xTrIon.shape[1]}')\n",
    "print('Training set: (n x d matrix)')\n",
    "TrIon_for_display = np.concatenate([yTrIon[:, None], xTrIon], axis=1)\n",
    "TrIon_for_display = TrIon_for_display[TrIon_for_display[:, 0].argsort()]\n",
    "\n",
    "display(pd.DataFrame(data=TrIon_for_display,\n",
    "                     columns=['$y$'] + [f'$[\\mathbf{{x}}]_{ {i+1} }$' for i in range(xTrIon.shape[1])]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1471471f16e7153e9f7916d8893fab91",
     "grade": false,
     "grade_id": "cell-c632b2561da0c25c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part One: Implement `sqimpurity` [Graded]\n",
    "\n",
    "First, implement the function **`sqimpurity`**, which takes as input a vector $y$ of $n$ labels and outputs the corresponding squared loss impurity:\n",
    "$$\n",
    "\\sum_{i = 1}^{n} \\left( y_i - \\overline{y} \\right)^2 \\textrm{, where } \\overline{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}.\n",
    "$$\n",
    "\n",
    "Again, the squared loss impurity works fine even though our final objective is classification. This is because the labels are binary and classification problems can be framed as regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "def430a6952c11c5e4cf8b3d9516fe81",
     "grade": false,
     "grade_id": "cell-ec2301f1325f79b5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqimpurity(yTr):\n",
    "    \"\"\"\n",
    "    Computes the squared loss impurity (variance) of the labels.\n",
    "    \n",
    "    Input:\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        squared loss impurity: weighted variance/squared loss impurity of the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    N, = yTr.shape\n",
    "    assert N > 0 # must have at least one sample\n",
    "    impurity = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    impurity=np.sum(np.square(yTr-np.mean(yTr)))\n",
    "    # raise NotImplementedError()\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28e61f2dc5aeb81315a4e40b9306e92d",
     "grade": false,
     "grade_id": "cell-ba20a92528e16fbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: sqimpurity_test1 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test2 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test3 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test4 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test5 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "def sqimpurity_test1():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isscalar(impurity)  # impurity should be scalar\n",
    "\n",
    "def sqimpurity_test2():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return impurity >= 0 # impurity should be nonnegative\n",
    "\n",
    "def sqimpurity_test3():\n",
    "    yTr = np.ones(100) # generate an all one vector as labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isclose(impurity, 0) # impurity should be zero since the labels are homogeneous\n",
    "\n",
    "def sqimpurity_test4():\n",
    "    yTr = np.arange(-5, 6) # generate a vector with mean zero\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    sum_of_squares = np.sum(yTr ** 2) \n",
    "    return np.isclose(impurity, sum_of_squares) # with mean zero, then the impurity should be the sum of squares\n",
    "\n",
    "def sqimpurity_test5():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr)\n",
    "    impurity_grader = sqimpurity_grader(yTr)\n",
    "    return np.isclose(impurity, impurity_grader)\n",
    "\n",
    "runtest(sqimpurity_test1, 'sqimpurity_test1')\n",
    "runtest(sqimpurity_test2, 'sqimpurity_test2')\n",
    "runtest(sqimpurity_test3, 'sqimpurity_test3')\n",
    "runtest(sqimpurity_test4, 'sqimpurity_test4')\n",
    "runtest(sqimpurity_test5, 'sqimpurity_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e4b6cdeffb9e6abb0e20f1426d5e335",
     "grade": true,
     "grade_id": "cell-84a2790fead7b76f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "295c7921f17373bcee865218851434f0",
     "grade": true,
     "grade_id": "cell-835351931df18818",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "15be06d818d669ee3d72cd31f9849813",
     "grade": true,
     "grade_id": "cell-3c3b7a31a818b505",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "465b390daf35d5c677b377e4a98633ee",
     "grade": true,
     "grade_id": "cell-1c7dc9e1879e6a32",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0906343847ad8e1e8800b1c85bd474a7",
     "grade": true,
     "grade_id": "cell-330bca5d42fef37a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "110d11c6a9b86f38e65eb9d517778dac",
     "grade": false,
     "grade_id": "cell-c8238fba4c3de7ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's plot the shape of the impurity function. We vary the mixture of labels in a set of $n$ labels and calculate the impurity of the labels. When the labels are mostly the same, the impurity should be low. When the labels are evenly split between $+1$ and $-1$, the impurity should be the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraction_pos</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.2</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fraction_pos  impurity\n",
       "0            1.0       0.0\n",
       "1            0.9       3.6\n",
       "2            0.8       6.4\n",
       "3            0.7       8.4\n",
       "4            0.6       9.6\n",
       "5            0.5      10.0\n",
       "6            0.4       9.6\n",
       "7            0.3       8.4\n",
       "8            0.2       6.4\n",
       "9            0.1       3.6\n",
       "10           0.0       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Squared loss impurity of labels')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 10\n",
    "y = np.ones(size)\n",
    "fraction_pos, impurities = [], []\n",
    "for i in range(size):\n",
    "    fraction_pos.append(sum([y == 1]) / size)\n",
    "    impurities.append(sqimpurity(y))\n",
    "    y[i] = -1\n",
    "fraction_pos.append(sum([y == 1]) / size)\n",
    "impurities.append(sqimpurity(y))\n",
    "\n",
    "display(pd.DataFrame(data={'fraction_pos': fraction_pos, 'impurity': impurities}))\n",
    "plt.plot(fraction_pos, impurities)\n",
    "plt.grid()\n",
    "plt.xlabel('Fraction of positive labels (1 - fraction of negative labels)')\n",
    "plt.ylabel('Squared loss impurity of labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "27c84f41b6f8c383bd75fccc138ad42c",
     "grade": false,
     "grade_id": "cell-4730d6e94ace8e06",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two: Implement `sqsplit` [Graded]\n",
    "\n",
    "Now implement **`sqsplit`**, which takes as input a data set of size $n \\times d$ with labels and computes the best feature and the threshold/cut of the optimal split based on the squared loss impurity. The function outputs a feature dimension `0 <= feature < d`, a cut threshold `cut`, and the impurity loss `bestloss` of this best split.\n",
    "\n",
    "Recall in the CART algorithm that, to find the split with the minimum impurity, you iterate over all features and cut values along each feature. We enforce that the cut value be the average of the two consecutive data points' feature values.\n",
    "\n",
    "You should calculate the impurity of a node of data $S$ with two branches $S_L$ and $S_R$ as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(S) &= \\frac{\\left| S_L \\right|}{|S|} I \\left( S_L \\right) + \\frac{\\left| S_R \\right|}{|S|} I \\left( S_R \\right)\\\\\n",
    "&= \\frac{1}{|S|}\\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\frac{1}{|S|} \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\\\\\n",
    "&\\propto \\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Implementation Notes:**\n",
    "- For calculating the impurity of a node, you should just return the sum of left and right impurities instead of the average.\n",
    "- Returned `feature` must be 0-indexed as is consistent with programming in Python.\n",
    "- If along a feature $f$, two data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ have the same value, avoid splitting between them; move to the next pair of data points.\n",
    "\n",
    "For example, with the following `xTr` of size $4 \\times 3$ and `yTr` for 4 points:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2\\\\\n",
    "2 & 0 & 1\\\\\n",
    "0 & 0 & 1\\\\\n",
    "2 & 1 & 2\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1\\\\1\\\\1\\\\-1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "among possible features `[0, 1, 2]`, the best split would be at `feature = 1` and `cut = (0 + 1) / 2 = 0.5`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "If you're stuck, we recommend that you start with the naïve algorithm for finding the best split, which involves a double loop over all features `0 <= f < d` and all cut values `xTr[0, f] < (xTr[i, f] + xTr[i+1, f]) / 2 < xTr[n-1, f]` (with `xTr` sorted along feature `f`). This algorithm thus calculates impurities for `d(n-1)` splits. Here's the pseudocode:\n",
    "\n",
    "<center><img src=\"cart-id3_best_split_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d9033ab9fb3dbbcb866dcc1bd45db8f4",
     "grade": false,
     "grade_id": "cell-sqsplit",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqsplit(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Finds the best feature, cut value, and impurity for a split of (xTr, yTr) based on squared loss impurity.\n",
    "    \n",
    "    Input:\n",
    "        xTr: n x d matrix of data points\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature (keep in mind this is 0-indexed)\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: squared loss impurity of the best cut\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    assert d > 0 # must have at least one dimension\n",
    "    assert n > 1 # must have at least two samples\n",
    "    \n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    for col in range(d):\n",
    "        \n",
    "        sortIdx = xTr[:,col].argsort()      # sort data along that dimensions\n",
    "        xsorted = xTr[sortIdx,col]          # sorted feature values\n",
    "        ysorted = yTr[sortIdx]              # sorted labels\n",
    "    \n",
    "        \n",
    "        for row in range(n):\n",
    "            if row + 1 == n:   # if row + 1 exceeds range\n",
    "                continue\n",
    "                \n",
    "            # check where values change\n",
    "                \n",
    "            if xsorted[row + 1] > xsorted[row]:\n",
    "                Sl = ysorted[:row+1]\n",
    "                Sr = ysorted[row+1:]\n",
    "                \n",
    "                loss=sqimpurity(Sl)+sqimpurity(Sr)\n",
    "            \n",
    "                if loss < bestloss:\n",
    "                    bestloss=loss\n",
    "                    feature=col\n",
    "                    cut =(xsorted[row]+xsorted[row + 1])/2\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    return feature, cut, bestloss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edbc3579da8f3891cc0b907caf6f397f",
     "grade": false,
     "grade_id": "cell-sqsplit_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.58 seconds\n",
      "The best split is on feature 2 on value 0.304\n",
      "Your tree split on feature 2 on value: 0.304 \n",
      "\n",
      "Running Test: sqsplit_test1 ... ✔ Passed!\n",
      "Running Test: sqsplit_test2 ... ✔ Passed!\n",
      "Running Test: sqsplit_test3 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# The tests below check that your sqsplit function returns the correct values for several different input datasets\n",
    "\n",
    "t0 = time.time()\n",
    "fid, cut, loss = sqsplit(xTrIon,yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {:0.2f} seconds'.format(t1-t0))\n",
    "print(\"The best split is on feature 2 on value 0.304\")\n",
    "print(\"Your tree split on feature %i on value: %2.3f \\n\" % (fid,cut))\n",
    "\n",
    "def sqsplit_test1():\n",
    "    a = np.isclose(sqsplit(xor4, yor4)[2] / len(yor4), .25)\n",
    "    b = np.isclose(sqsplit(xor3, yor3)[2] / len(yor3), .25)\n",
    "    c = np.isclose(sqsplit(xor2, yor2)[2] / len(yor2), .25)\n",
    "    return a and b and c\n",
    "\n",
    "def sqsplit_test2():\n",
    "    x = np.array(range(1000)).reshape(-1,1)\n",
    "    y = np.hstack([np.ones(500),-1*np.ones(500)]).T\n",
    "    _, cut, _ = sqsplit(x, y)\n",
    "    return cut <= 500 or cut >= 499\n",
    "\n",
    "def sqsplit_test3():\n",
    "    fid, cut, loss = sqsplit(xor5,yor5)\n",
    "    # cut should be 0.5 but 0 is also accepted\n",
    "    return fid == 0 and (cut >= 0 or cut <= 1) and np.isclose(loss / len(yor5), 2/3)\n",
    "\n",
    "runtest(sqsplit_test1,'sqsplit_test1')\n",
    "runtest(sqsplit_test2,'sqsplit_test2')\n",
    "runtest(sqsplit_test3,'sqsplit_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cbffe6477a05dd8d9db574f0d94edcfa",
     "grade": true,
     "grade_id": "cell-sqsplit_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0b0da843388d7a9ddf59c80d408d3859",
     "grade": true,
     "grade_id": "cell-sqsplit_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86d84abb3781b6b263676a20c5ed6418",
     "grade": true,
     "grade_id": "cell-sqsplit_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d358dfc0f25657b18024284428aec959",
     "grade": false,
     "grade_id": "cell-985b43cf6a0b1e16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Three: Implement `cart` [Graded]\n",
    "\n",
    "In this section, you will implement the function **`cart`**, which returns a regression tree based on the minimum squared loss splitting rule. You should use the function `sqsplit` to make your splits.\n",
    "\n",
    "**Implementation Notes:**\n",
    "We've provided a tree structure in the form of `TreeNode` for you that can be used for both leaves and nodes. To represent the leaves, you would set all fields except `prediction` to `None`.\n",
    "\n",
    "Non-leaf nodes will have non-`None` fields for all except `prediction`:\n",
    "1. `left`: node describing left subtree\n",
    "2. `right`: node describing right subtree\n",
    "3. `feature`: index of feature to cut (0-indexed as returned by `sqsplit`)\n",
    "4. `cut`: cutoff value $t$ ($\\leq t$: left and $> t$: right)\n",
    "5. `prediction`: `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0903afbf0f71752d3ae3caa2a24e8360",
     "grade": false,
     "grade_id": "cell-919899ceb491184f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"\n",
    "    Tree class.\n",
    "    \n",
    "    (You don't _need_ to add any methods or fields here but feel\n",
    "    free to if you like. The tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, feature, cut, prediction):\n",
    "        # Check that all or no arguments are None\n",
    "        node_or_leaf_args = [left, right, feature, cut]\n",
    "        assert all([arg == None for arg in node_or_leaf_args]) or all([arg != None for arg in node_or_leaf_args])\n",
    "        \n",
    "        # Check that all None <==> leaf <==> prediction not None\n",
    "        # Check that all non-None <==> non-leaf <==> prediction is None\n",
    "        if all([arg == None for arg in node_or_leaf_args]):\n",
    "            assert prediction is not None\n",
    "        if all([arg != None for arg in node_or_leaf_args]):\n",
    "            assert prediction is None\n",
    "        \n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.feature = feature \n",
    "        self.cut = cut\n",
    "        self.prediction = prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7eac0b2166f8fddde8c51b4ab8470e47",
     "grade": false,
     "grade_id": "cell-5b554dfec9394ba9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell contains some examples of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a tree that predicts everything as zero ==> prediction 0\n",
    "# In this case, it has no left or right children (it is a leaf node) ==> left = None, right = None, feature = None, cut = None\n",
    "root = TreeNode(None, None, None, None, 0)\n",
    "\n",
    "\n",
    "# The following that a tree with depth 2 or a tree with one split \n",
    "\n",
    "# The tree will return a prediction of 1 if an example falls under the left subtree\n",
    "# Otherwise it will return a prediction of 2\n",
    "# To start, first create two leaf node\n",
    "left_leaf = TreeNode(None, None, None, None, 1)\n",
    "right_leaf = TreeNode(None, None, None, None, 2)\n",
    "\n",
    "# Now create the parent or the root\n",
    "# Suppose we split at feature 0 and cut of 1 and the prediction is None\n",
    "root2 = TreeNode(left_leaf, right_leaf, 0, 1, None)\n",
    "\n",
    "# Now root2 is the tree we desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9c2c5f1f300c14fa009977cd29781d7c",
     "grade": false,
     "grade_id": "cell-d6c63e37fd6c395f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the function `cart` using **recursion** (you call `cart` on the left and right subtrees inside the `cart` function). Recall the pseudocode for the CART algorithm.\n",
    "\n",
    "**NOTE:** In this implementation, you will be using **`np.mean`** for `prediction` argument. To check that floating point values in `xTr` are the same or not, you can use `np.isclose(xTr, xTr[0])`, which returns a list of `True` and `False` based on how different the rows of `xTr` are from the vector `xTr[0]`.\n",
    "\n",
    "<center><img src=\"cart-id3_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de3750e532687607733d83dcc9a0a2dc",
     "grade": false,
     "grade_id": "cell-cart",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cart(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Builds a CART tree.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data\n",
    "        yTr:      n-dimensional vector\n",
    "\n",
    "    Output:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    node = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    prediction = np.mean(yTr)\n",
    "    idx = np.arange(n)\n",
    "    \n",
    "    #if node is leaf\n",
    "    if np.all(yTr == yTr[0]) or np.max(np.abs(np.diff(xTr, axis=0))) < 1e-5:\n",
    "        tree = TreeNode(None, None, None, None, prediction)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        feature, cut, loss = sqsplit(xTr, yTr)\n",
    "          \n",
    "        left_idx = idx[xTr[:,feature] <= cut]\n",
    "        right_idx = idx[xTr[:,feature] > cut]\n",
    "        left_xTr = xTr[left_idx,:]\n",
    "        right_xTr = xTr[right_idx,:]\n",
    "        left_yTr = yTr[left_idx]\n",
    "        right_yTr = yTr[right_idx]\n",
    "    \n",
    "        left_node = cart(left_xTr,left_yTr)\n",
    "        right_node = cart(right_xTr,right_yTr)\n",
    "        tree = TreeNode(left_node, right_node, feature, cut, prediction=None)  \n",
    "    \n",
    "    return tree\n",
    "    ## raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc022deb75c9f2be5071b44c001e6d82",
     "grade": false,
     "grade_id": "cell-cart_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: cart_test1 ... ✔ Passed!\n",
      "Running Test: cart_test2 ... ✔ Passed!\n",
      "Running Test: cart_test3 ... ✔ Passed!\n",
      "Running Test: cart_test4 ... ✔ Passed!\n",
      "Running Test: cart_test5 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# The tests below check that your implementation of cart  returns the correct predicted values for a sample dataset\n",
    "\n",
    "#test case 1\n",
    "def cart_test1():\n",
    "    t=cart(xor4,yor4)\n",
    "    return DFSxor(t)\n",
    "\n",
    "#test case 2\n",
    "def cart_test2():\n",
    "    y = np.random.rand(16);\n",
    "    t = cart(xor4,y);\n",
    "    yTe = DFSpreds(t)[:];\n",
    "    # Check that every label appears exactly once in the tree\n",
    "    y.sort()\n",
    "    yTe.sort()\n",
    "    return np.all(np.isclose(y, yTe))\n",
    "\n",
    "#test case 3\n",
    "def cart_test3():\n",
    "    xRep = np.concatenate([xor2, xor2])\n",
    "    yRep = np.concatenate([yor2, 1-yor2])\n",
    "    t = cart(xRep, yRep)\n",
    "    return DFSxorUnsplittable(t)\n",
    "\n",
    "#test case 4\n",
    "def cart_test4():\n",
    "    X = np.ones((5, 2)) # Create a dataset with identical examples\n",
    "    y = np.ones(5)\n",
    "    \n",
    "    # On this dataset, your cart algorithm should return a single leaf\n",
    "    # node with prediction equal to 1\n",
    "    t = cart(X, y)\n",
    "    \n",
    "    # t has no children\n",
    "    children_check = (t.left is None) and (t.right is None) \n",
    "    \n",
    "    # Make sure t does not cut any feature and at any value\n",
    "    feature_check = (t.feature is None) and (t.cut is None)\n",
    "    \n",
    "    # Check t's prediction\n",
    "    prediction_check = np.isclose(t.prediction, 1)\n",
    "    return children_check and feature_check and prediction_check\n",
    "\n",
    "#test case 5\n",
    "def cart_test5():\n",
    "    X = np.arange(4).reshape(-1, 1)\n",
    "    y = np.array([0, 0, 1, 1])\n",
    "\n",
    "    t = cart(X, y) # your cart algorithm should generate one split\n",
    "    \n",
    "    # check whether you set t.feature and t.cut to something\n",
    "    return t.feature is not None and t.cut is not None\n",
    "\n",
    "\n",
    "\n",
    "runtest(cart_test1,'cart_test1')\n",
    "runtest(cart_test2,'cart_test2')\n",
    "runtest(cart_test3,'cart_test3')\n",
    "runtest(cart_test4,'cart_test4')\n",
    "runtest(cart_test5,'cart_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b47e3031bb2beb61cae1c347d53b283",
     "grade": true,
     "grade_id": "cell-cart_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e322a7a1bab0a83d65ad27517c275cab",
     "grade": true,
     "grade_id": "cell-cart_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d6bf0051fdf9a7296e893eb66c16b96",
     "grade": true,
     "grade_id": "cell-cart_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07b92c1bbd6474aa04712ddb4dbcc0ca",
     "grade": true,
     "grade_id": "cell-cart_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e3a55c57681ae193e2771512f553e528",
     "grade": true,
     "grade_id": "cell-cart_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "faf306bddae40ade7985dcf6439b861c",
     "grade": false,
     "grade_id": "cell-91bac7e5c5968bbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Four: Implement `evaltree` [Graded]\n",
    "\n",
    "Implement the function **`evaltree`**, which evaluates a decision tree on a given test data set. You essentially need to traverse the tree until you end up in a leaf, where you return the `prediction` value of the leaf. Like the `cart` function, you can call `evaltree` on the left subtree and right subtree on testing points that fall in the corresponding subtrees.\n",
    "\n",
    "Here's some inspiration:\n",
    "1. If the `tree` is a leaf, i.e. the left and right subtrees are `None`, return `tree.prediction` for all `m` testing points.\n",
    "2. If the `tree` is non-leaf, using `tree.feature` and `tree.cut` find testing points with the feature value less than/equal to the threshold and greater than. Now, you can call `evaltree` on `tree.left` and the left set of testing points to obtain the left set's predictions. Then obtain the predictions for the right set, and return all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54f3f523c3529265fb0ce24f6fea9361",
     "grade": false,
     "grade_id": "cell-evaltree",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evaltree(tree, xTe):\n",
    "    \"\"\"\n",
    "    Evaluates testing points in xTe using decision tree root.\n",
    "    \n",
    "    Input:\n",
    "        tree: TreeNode decision tree\n",
    "        xTe:  m x d matrix of data points\n",
    "    \n",
    "    Output:\n",
    "        pred: m-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m, d = xTe.shape\n",
    "    preds = np.zeros(m)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59551843c4d5ab8b8c0924087bb9202c",
     "grade": false,
     "grade_id": "cell-evaltree_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7d5e4a75b8bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtr_err\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaltree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxTrIon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myTrIon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mte_err\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaltree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxTeIon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0myTeIon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-c09c1930764b>\u001b[0m in \u001b[0;36mevaltree\u001b[0;34m(tree, xTe)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The following tests check that your implementation of evaltree returns the correct predictions for two sample trees\n",
    "\n",
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)\n",
    "\n",
    "#test case 1\n",
    "def evaltree_test1():\n",
    "    t = cart(xor4,yor4)\n",
    "    xor4te = xor4 + (np.sign(xor4 - .5) * .1)\n",
    "    inds = np.arange(16)\n",
    "    np.random.shuffle(inds)\n",
    "    # Check that shuffling and expanding the data doesn't affect the predictions\n",
    "    return np.all(np.isclose(evaltree(t, xor4te[inds,:]), yor4[inds]))\n",
    "\n",
    "#test case 2\n",
    "def evaltree_test2():\n",
    "    a = TreeNode(None, None, None, None, 1)\n",
    "    b = TreeNode(None, None, None, None, -1)\n",
    "    c = TreeNode(None, None, None, None, 0)\n",
    "    d = TreeNode(None, None, None, None, -1)\n",
    "    e = TreeNode(None, None, None, None, -1)\n",
    "    x = TreeNode(a, b, 0, 10, None)\n",
    "    y = TreeNode(x, c, 0, 20, None)\n",
    "    z = TreeNode(d, e, 0, 40, None)\n",
    "    t = TreeNode(y, z, 0, 30, None)\n",
    "    # Check that the custom tree evaluates correctly\n",
    "    return np.all(np.isclose(\n",
    "            evaltree(t, np.array([[45, 35, 25, 15, 5]]).T),\n",
    "            np.array([-1, -1, 0, -1, 1])))\n",
    "\n",
    "runtest(evaltree_test1,'evaltree_test1')\n",
    "runtest(evaltree_test2,'evaltree_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d772deb41878ca9f83b8c3661ac75b67",
     "grade": true,
     "grade_id": "cell-evaltree_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "68d89de2748bcdc384eb209588ffb84d",
     "grade": true,
     "grade_id": "cell-evaltree_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dfac59f41d855c0f8b4e1e7bf394c7d",
     "grade": false,
     "grade_id": "cell-030c38de62d6adc6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Training and Testing a Classification Tree on the ION Dataset</h3>\n",
    "\n",
    "<p> The following code create a classification tree on the ION dataset and then apply the learned tree to an unknown dataset. If you implement everything correctly, you should get a training RSME that is close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "efc7b56e0189a0d123d3ac40b1011afa",
     "grade": false,
     "grade_id": "cell-534ce4bb724b2f26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Visualize Your Tree</h3>\n",
    "\n",
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the ion data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "694a973d8d7c2709f769a08c64cf8f0f",
     "grade": false,
     "grade_id": "cell-8059df3cc7cdb39d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr,w=None,b=0):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "\n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    \n",
    "    if w is not None:\n",
    "        w = np.array(w).flatten()\n",
    "        alpha = -1 * b / (w ** 2).sum()\n",
    "        plt.quiver(w[0] * alpha, w[1] * alpha,\n",
    "            w[0], w[1], linewidth=2, color=[0,1,0])\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "\n",
    "tree=cart(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X:evaltree(tree,X), xTrSpiral, yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evaltree(tree,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evaltree(tree,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f1fe9d13867759f9403c562fe900853a",
     "grade": false,
     "grade_id": "cell-8ad51604f6c3e28c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def onclick_cart(event):\n",
    "    \"\"\"\n",
    "    Visualize cart, including new point\n",
    "    \"\"\"\n",
    "    global xTraining,labels\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTraining = np.concatenate((xTraining,pos), axis = 0)\n",
    "    labels.append(label);\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(labels)\n",
    "        \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get decision tree\n",
    "    tree=cart(xTraining,np.array(labels).flatten())\n",
    "    fun = lambda X:evaltree(tree,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTraining[labels == c,0],\n",
    "            xTraining[labels == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "\n",
    "%matplotlib notebook\n",
    "xTraining= np.array([[5,6]])\n",
    "labels = [1]\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_cart)\n",
    "plt.title('Click to add positive points and use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "\n",
    "Scikit-learn also provides an implementation of [Regression Trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) (and [Decision Tree Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). The usage is pretty straight-forward: define the regression tree with the impurity function (and other settings), fit to the training set, and evaluate on any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "t0 = time.time()\n",
    "tree = DecisionTreeRegressor(\n",
    "    criterion='mse', # Impurity function = Mean Squared Error (squared loss)\n",
    "    splitter='best', # Take the best split\n",
    "    max_depth=None, # Expand the tree to the maximum depth possible\n",
    ")\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((tree.predict(xTrSpiral) - yTrSpiral)**2)\n",
    "te_err   = np.mean((tree.predict(xTeSpiral) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also provides a tree plotting function, which is again quite simple to use. This is extremely useful while debugging a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "_ = plot_tree(tree, ax=ax, precision=2, feature_names=[f'$[\\mathbf{{x}}]_{i+1}$' for i in range(2)], filled=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
