{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02ea6ea566df7354a9a88bcce4c6e714",
     "grade": false,
     "grade_id": "cell-202739b8f3a67536",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "\n",
    "In this project, you will implement the CART (Classification and Regression Tree) algorithm. You will work with the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">Ionosphere Data Set from the UCI Machine Learning Repository</a>, which consists of radar data from a system designed to target free electrons in the ionosphere, and a artificial \"spiral\" dataset. The first dataset will be used to determine if a radar return was \"good\" (i.e. a signal was returned) or \"bad\" (i.e. the signal passed straight through the ionosphere).\n",
    "\n",
    "**You will be using a regression tree with squared loss impurity to do classification. This is possible here because all classification problems can be framed as regression problems. You could also have used a classification tree with Gini impurity equivalently.**\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf891c1be4dc1ae06191273f05e0d68e",
     "grade": false,
     "grade_id": "cell-727395f9bc8b5c9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Implementing Regression Trees</h2>\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before you get started, let's import a few packages that you will need. In addition, you will load two binary classification dataset - the spiral dataset and the <a href=\"https://archive.ics.uci.edu/ml/datasets/Ionosphere\">ION</a> dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ad00e0439033b41698216da17731123",
     "grade": false,
     "grade_id": "cell-5445d0afdbc4c054",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.6.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a36ccf5bfced96b28d8966f36446473",
     "grade": false,
     "grade_id": "cell-c219552fd154672d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The code below generates spiral data using the trigonometric functions sine and cosine, then splits the data into train and test segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9495e5cadae1976dadd4b1ad0e63367c",
     "grade": false,
     "grade_id": "cell-9a38f0686e9d323f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points in spiral dataset: 150\n",
      "Number of testing points in spiral dataset: 150\n",
      "Number of features in spiral dataset: 2\n"
     ]
    }
   ],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N) # generate a vector of \"radius\" values\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T # generate a curve that draws circles with increasing radius\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    # Now sample alternating values to generate the test and train sets.\n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr, yTr, xTe, yTe\n",
    "\n",
    "xTrSpiral, yTrSpiral, xTeSpiral, yTeSpiral = spiraldata(150)\n",
    "print(f'Number of training points in spiral dataset: {xTrSpiral.shape[0]}')\n",
    "print(f'Number of testing points in spiral dataset: {xTeSpiral.shape[0]}')\n",
    "print(f'Number of features in spiral dataset: {xTrSpiral.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "66d9205e7c90b804b83fed62414fed1e",
     "grade": false,
     "grade_id": "cell-65f296cbe2f8b1ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Each data point $[\\mathbf{x}]_i$ in the spiral data has 2 dimensions and the label $y_i$ is either $-1$ or $+1$. We can plot `xTrSpiral` to see the points, colored by the label they are associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcfdaffee10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(xTrSpiral[:, 0], xTrSpiral[:, 1], s=30, c=yTrSpiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3183e35d16b85a5f474e39a15cd14f5e",
     "grade": false,
     "grade_id": "cell-d73f1d288cf74deb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following code loads the ION dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8223c9331576477c95d446780580ff1",
     "grade": false,
     "grade_id": "cell-134b99a8d0c69131",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points in ION dataset: 281\n",
      "Number of testing points in ION dataset: 70\n",
      "Number of features in ION dataset: 34\n",
      "Training set: (n x d matrix)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$y$</th>\n",
       "      <th>$[\\mathbf{x}]_{1}$</th>\n",
       "      <th>$[\\mathbf{x}]_{2}$</th>\n",
       "      <th>$[\\mathbf{x}]_{3}$</th>\n",
       "      <th>$[\\mathbf{x}]_{4}$</th>\n",
       "      <th>$[\\mathbf{x}]_{5}$</th>\n",
       "      <th>$[\\mathbf{x}]_{6}$</th>\n",
       "      <th>$[\\mathbf{x}]_{7}$</th>\n",
       "      <th>$[\\mathbf{x}]_{8}$</th>\n",
       "      <th>$[\\mathbf{x}]_{9}$</th>\n",
       "      <th>...</th>\n",
       "      <th>$[\\mathbf{x}]_{25}$</th>\n",
       "      <th>$[\\mathbf{x}]_{26}$</th>\n",
       "      <th>$[\\mathbf{x}]_{27}$</th>\n",
       "      <th>$[\\mathbf{x}]_{28}$</th>\n",
       "      <th>$[\\mathbf{x}]_{29}$</th>\n",
       "      <th>$[\\mathbf{x}]_{30}$</th>\n",
       "      <th>$[\\mathbf{x}]_{31}$</th>\n",
       "      <th>$[\\mathbf{x}]_{32}$</th>\n",
       "      <th>$[\\mathbf{x}]_{33}$</th>\n",
       "      <th>$[\\mathbf{x}]_{34}$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.83</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>1.91</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.10</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.67</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.81</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-1.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.73</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.23</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>1.85</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.70</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.82</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>1.39</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>1.22</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.37</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     $y$  $[\\mathbf{x}]_{1}$  $[\\mathbf{x}]_{2}$  $[\\mathbf{x}]_{3}$  \\\n",
       "0   -1.0                0.55                1.66               -0.56   \n",
       "1   -1.0                0.23               -0.04                1.61   \n",
       "2   -1.0                2.18               -1.05                0.27   \n",
       "3   -1.0               -2.10                1.56                0.55   \n",
       "4   -1.0                1.55                0.18                0.74   \n",
       "..   ...                 ...                 ...                 ...   \n",
       "276  1.0                3.37               -1.57               -1.80   \n",
       "277  1.0               -0.05                0.03                0.39   \n",
       "278  1.0               -1.34               -0.13               -0.36   \n",
       "279  1.0               -1.24                0.13               -0.09   \n",
       "280  1.0               -2.00               -0.19               -0.29   \n",
       "\n",
       "     $[\\mathbf{x}]_{4}$  $[\\mathbf{x}]_{5}$  $[\\mathbf{x}]_{6}$  \\\n",
       "0                  1.01                1.83               -0.46   \n",
       "1                 -0.09               -0.23               -1.59   \n",
       "2                  2.20                2.15               -2.48   \n",
       "3                 -1.53               -0.92               -0.31   \n",
       "4                  0.78                0.18                0.38   \n",
       "..                  ...                 ...                 ...   \n",
       "276               -2.17                0.13                0.04   \n",
       "277                0.11               -0.05                0.09   \n",
       "278                0.46                1.19                0.07   \n",
       "279               -0.27                0.06               -0.03   \n",
       "280               -0.09               -0.04               -0.03   \n",
       "\n",
       "     $[\\mathbf{x}]_{7}$  $[\\mathbf{x}]_{8}$  $[\\mathbf{x}]_{9}$  ...  \\\n",
       "0                  1.91               -1.48               -1.15  ...   \n",
       "1                  1.35                0.34               -0.03  ...   \n",
       "2                 -0.13                1.76                0.14  ...   \n",
       "3                  2.06               -0.30               -0.15  ...   \n",
       "4                 -0.10               -0.21               -0.63  ...   \n",
       "..                  ...                 ...                 ...  ...   \n",
       "276                0.32                0.10               -0.40  ...   \n",
       "277               -0.10                0.17                0.16  ...   \n",
       "278                0.34               -0.38                0.23  ...   \n",
       "279               -0.32                0.09                0.17  ...   \n",
       "280                0.07                0.08               -0.07  ...   \n",
       "\n",
       "     $[\\mathbf{x}]_{25}$  $[\\mathbf{x}]_{26}$  $[\\mathbf{x}]_{27}$  \\\n",
       "0                  -0.51                -0.46                 0.58   \n",
       "1                   0.36                -0.24                 0.06   \n",
       "2                   0.02                 0.27                 0.08   \n",
       "3                  -0.53                 0.30                -0.74   \n",
       "4                   0.01                 0.19                -0.18   \n",
       "..                   ...                  ...                  ...   \n",
       "276                 0.29                 0.14                 0.06   \n",
       "277                -0.09                -0.06                 0.07   \n",
       "278                -0.24                 0.07                 0.18   \n",
       "279                 0.09                -0.06                -0.02   \n",
       "280                 0.00                -0.03                 0.01   \n",
       "\n",
       "     $[\\mathbf{x}]_{28}$  $[\\mathbf{x}]_{29}$  $[\\mathbf{x}]_{30}$  \\\n",
       "0                   0.30                 0.19                -0.30   \n",
       "1                  -0.21                -0.21                -0.04   \n",
       "2                   0.15                -0.22                 0.09   \n",
       "3                   0.03                -0.14                 0.23   \n",
       "4                  -0.18                 0.02                -0.04   \n",
       "..                   ...                  ...                  ...   \n",
       "276                -0.47                 0.30                 0.07   \n",
       "277                -0.15                -0.04                -0.09   \n",
       "278                 0.06                 0.00                -0.66   \n",
       "279                 0.07                 0.04                -0.01   \n",
       "280                -0.02                -0.01                -0.01   \n",
       "\n",
       "     $[\\mathbf{x}]_{31}$  $[\\mathbf{x}]_{32}$  $[\\mathbf{x}]_{33}$  \\\n",
       "0                  -0.48                -0.45                -0.09   \n",
       "1                   0.00                 0.22                 0.08   \n",
       "2                  -0.34                -0.45                -0.15   \n",
       "3                   0.00                -0.18                -0.02   \n",
       "4                  -0.22                 0.19                 0.05   \n",
       "..                   ...                  ...                  ...   \n",
       "276                -0.54                 0.27                -0.12   \n",
       "277                 0.12                -0.15                -0.09   \n",
       "278                 0.11                 0.19                 0.13   \n",
       "279                -0.03                -0.05                -0.04   \n",
       "280                -0.01                -0.00                 0.03   \n",
       "\n",
       "     $[\\mathbf{x}]_{34}$  \n",
       "0                   -0.0  \n",
       "1                   -0.0  \n",
       "2                   -0.0  \n",
       "3                    0.0  \n",
       "4                   -0.0  \n",
       "..                   ...  \n",
       "276                  0.0  \n",
       "277                 -0.0  \n",
       "278                 -0.0  \n",
       "279                 -0.0  \n",
       "280                  0.0  \n",
       "\n",
       "[281 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in some binary test data (labels are -1, +1)\n",
    "data = loadmat(\"ion.mat\")\n",
    "\n",
    "# Load the training data\n",
    "xTrIon  = data['xTr'].T\n",
    "yTrIon  = data['yTr'].flatten()\n",
    "\n",
    "# Load the test data\n",
    "xTeIon  = data['xTe'].T\n",
    "yTeIon  = data['yTe'].flatten()\n",
    "\n",
    "print(f'Number of training points in ION dataset: {xTrIon.shape[0]}')\n",
    "print(f'Number of testing points in ION dataset: {xTeIon.shape[0]}')\n",
    "print(f'Number of features in ION dataset: {xTrIon.shape[1]}')\n",
    "print('Training set: (n x d matrix)')\n",
    "TrIon_for_display = np.concatenate([yTrIon[:, None], xTrIon], axis=1)\n",
    "TrIon_for_display = TrIon_for_display[TrIon_for_display[:, 0].argsort()]\n",
    "\n",
    "display(pd.DataFrame(data=TrIon_for_display,\n",
    "                     columns=['$y$'] + [f'$[\\mathbf{{x}}]_{ {i+1} }$' for i in range(xTrIon.shape[1])]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1471471f16e7153e9f7916d8893fab91",
     "grade": false,
     "grade_id": "cell-c632b2561da0c25c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part One: Implement `sqimpurity` [Graded]\n",
    "\n",
    "First, implement the function **`sqimpurity`**, which takes as input a vector $y$ of $n$ labels and outputs the corresponding squared loss impurity:\n",
    "$$\n",
    "\\sum_{i = 1}^{n} \\left( y_i - \\overline{y} \\right)^2 \\textrm{, where } \\overline{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}.\n",
    "$$\n",
    "\n",
    "Again, the squared loss impurity works fine even though our final objective is classification. This is because the labels are binary and classification problems can be framed as regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "def430a6952c11c5e4cf8b3d9516fe81",
     "grade": false,
     "grade_id": "cell-ec2301f1325f79b5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqimpurity(yTr):\n",
    "    \"\"\"\n",
    "    Computes the squared loss impurity (variance) of the labels.\n",
    "    \n",
    "    Input:\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        squared loss impurity: weighted variance/squared loss impurity of the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    N, = yTr.shape\n",
    "    assert N > 0 # must have at least one sample\n",
    "    impurity = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    impurity=np.sum(np.square(yTr-np.mean(yTr)))\n",
    "    # raise NotImplementedError()\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28e61f2dc5aeb81315a4e40b9306e92d",
     "grade": false,
     "grade_id": "cell-ba20a92528e16fbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: sqimpurity_test1 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test2 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test3 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test4 ... ✔ Passed!\n",
      "Running Test: sqimpurity_test5 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "def sqimpurity_test1():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isscalar(impurity)  # impurity should be scalar\n",
    "\n",
    "def sqimpurity_test2():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return impurity >= 0 # impurity should be nonnegative\n",
    "\n",
    "def sqimpurity_test3():\n",
    "    yTr = np.ones(100) # generate an all one vector as labels\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    return np.isclose(impurity, 0) # impurity should be zero since the labels are homogeneous\n",
    "\n",
    "def sqimpurity_test4():\n",
    "    yTr = np.arange(-5, 6) # generate a vector with mean zero\n",
    "    impurity = sqimpurity(yTr) # compute impurity\n",
    "    sum_of_squares = np.sum(yTr ** 2) \n",
    "    return np.isclose(impurity, sum_of_squares) # with mean zero, then the impurity should be the sum of squares\n",
    "\n",
    "def sqimpurity_test5():\n",
    "    yTr = np.random.randn(100) # generate random labels\n",
    "    impurity = sqimpurity(yTr)\n",
    "    impurity_grader = sqimpurity_grader(yTr)\n",
    "    return np.isclose(impurity, impurity_grader)\n",
    "\n",
    "runtest(sqimpurity_test1, 'sqimpurity_test1')\n",
    "runtest(sqimpurity_test2, 'sqimpurity_test2')\n",
    "runtest(sqimpurity_test3, 'sqimpurity_test3')\n",
    "runtest(sqimpurity_test4, 'sqimpurity_test4')\n",
    "runtest(sqimpurity_test5, 'sqimpurity_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e4b6cdeffb9e6abb0e20f1426d5e335",
     "grade": true,
     "grade_id": "cell-84a2790fead7b76f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "295c7921f17373bcee865218851434f0",
     "grade": true,
     "grade_id": "cell-835351931df18818",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "15be06d818d669ee3d72cd31f9849813",
     "grade": true,
     "grade_id": "cell-3c3b7a31a818b505",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "465b390daf35d5c677b377e4a98633ee",
     "grade": true,
     "grade_id": "cell-1c7dc9e1879e6a32",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0906343847ad8e1e8800b1c85bd474a7",
     "grade": true,
     "grade_id": "cell-330bca5d42fef37a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs sqimpurity_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "110d11c6a9b86f38e65eb9d517778dac",
     "grade": false,
     "grade_id": "cell-c8238fba4c3de7ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's plot the shape of the impurity function. We vary the mixture of labels in a set of $n$ labels and calculate the impurity of the labels. When the labels are mostly the same, the impurity should be low. When the labels are evenly split between $+1$ and $-1$, the impurity should be the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraction_pos</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.2</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fraction_pos  impurity\n",
       "0            1.0       0.0\n",
       "1            0.9       3.6\n",
       "2            0.8       6.4\n",
       "3            0.7       8.4\n",
       "4            0.6       9.6\n",
       "5            0.5      10.0\n",
       "6            0.4       9.6\n",
       "7            0.3       8.4\n",
       "8            0.2       6.4\n",
       "9            0.1       3.6\n",
       "10           0.0       0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Squared loss impurity of labels')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 10\n",
    "y = np.ones(size)\n",
    "fraction_pos, impurities = [], []\n",
    "for i in range(size):\n",
    "    fraction_pos.append(sum([y == 1]) / size)\n",
    "    impurities.append(sqimpurity(y))\n",
    "    y[i] = -1\n",
    "fraction_pos.append(sum([y == 1]) / size)\n",
    "impurities.append(sqimpurity(y))\n",
    "\n",
    "display(pd.DataFrame(data={'fraction_pos': fraction_pos, 'impurity': impurities}))\n",
    "plt.plot(fraction_pos, impurities)\n",
    "plt.grid()\n",
    "plt.xlabel('Fraction of positive labels (1 - fraction of negative labels)')\n",
    "plt.ylabel('Squared loss impurity of labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "27c84f41b6f8c383bd75fccc138ad42c",
     "grade": false,
     "grade_id": "cell-4730d6e94ace8e06",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two: Implement `sqsplit` [Graded]\n",
    "\n",
    "Now implement **`sqsplit`**, which takes as input a data set of size $n \\times d$ with labels and computes the best feature and the threshold/cut of the optimal split based on the squared loss impurity. The function outputs a feature dimension `0 <= feature < d`, a cut threshold `cut`, and the impurity loss `bestloss` of this best split.\n",
    "\n",
    "Recall in the CART algorithm that, to find the split with the minimum impurity, you iterate over all features and cut values along each feature. We enforce that the cut value be the average of the two consecutive data points' feature values.\n",
    "\n",
    "You should calculate the impurity of a node of data $S$ with two branches $S_L$ and $S_R$ as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(S) &= \\frac{\\left| S_L \\right|}{|S|} I \\left( S_L \\right) + \\frac{\\left| S_R \\right|}{|S|} I \\left( S_R \\right)\\\\\n",
    "&= \\frac{1}{|S|}\\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\frac{1}{|S|} \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\\\\\n",
    "&\\propto \\sum_{(\\mathbf{x}, y) \\in S_L} \\left( y-\\overline{y}_{S_L} \\right)^2 + \\sum_{(\\mathbf{x}, y) \\in S_R} \\left( y - \\overline{y}_{S_R} \\right)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Implementation Notes:**\n",
    "- For calculating the impurity of a node, you should just return the sum of left and right impurities instead of the average.\n",
    "- Returned `feature` must be 0-indexed as is consistent with programming in Python.\n",
    "- If along a feature $f$, two data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ have the same value, avoid splitting between them; move to the next pair of data points.\n",
    "\n",
    "For example, with the following `xTr` of size $4 \\times 3$ and `yTr` for 4 points:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2\\\\\n",
    "2 & 0 & 1\\\\\n",
    "0 & 0 & 1\\\\\n",
    "2 & 1 & 2\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1\\\\1\\\\1\\\\-1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "among possible features `[0, 1, 2]`, the best split would be at `feature = 1` and `cut = (0 + 1) / 2 = 0.5`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "If you're stuck, we recommend that you start with the naïve algorithm for finding the best split, which involves a double loop over all features `0 <= f < d` and all cut values `xTr[0, f] < (xTr[i, f] + xTr[i+1, f]) / 2 < xTr[n-1, f]` (with `xTr` sorted along feature `f`). This algorithm thus calculates impurities for `d(n-1)` splits. Here's the pseudocode:\n",
    "\n",
    "<center><img src=\"cart-id3_best_split_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d9033ab9fb3dbbcb866dcc1bd45db8f4",
     "grade": false,
     "grade_id": "cell-sqsplit",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sqsplit(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Finds the best feature, cut value, and impurity for a split of (xTr, yTr) based on squared loss impurity.\n",
    "    \n",
    "    Input:\n",
    "        xTr: n x d matrix of data points\n",
    "        yTr: n-dimensional vector of labels\n",
    "    \n",
    "    Output:\n",
    "        feature:  index of the best cut's feature (keep in mind this is 0-indexed)\n",
    "        cut:      cut-value of the best cut\n",
    "        bestloss: squared loss impurity of the best cut\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    assert d > 0 # must have at least one dimension\n",
    "    assert n > 1 # must have at least two samples\n",
    "    \n",
    "    bestloss = np.inf\n",
    "    feature = np.inf\n",
    "    cut = np.inf\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    #return feature, cut, bestloss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edbc3579da8f3891cc0b907caf6f397f",
     "grade": false,
     "grade_id": "cell-sqsplit_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The tests below check that your sqsplit function returns the correct values for several different input datasets\n",
    "\n",
    "t0 = time.time()\n",
    "fid, cut, loss = sqsplit(xTrIon,yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {:0.2f} seconds'.format(t1-t0))\n",
    "print(\"The best split is on feature 2 on value 0.304\")\n",
    "print(\"Your tree split on feature %i on value: %2.3f \\n\" % (fid,cut))\n",
    "\n",
    "def sqsplit_test1():\n",
    "    a = np.isclose(sqsplit(xor4, yor4)[2] / len(yor4), .25)\n",
    "    b = np.isclose(sqsplit(xor3, yor3)[2] / len(yor3), .25)\n",
    "    c = np.isclose(sqsplit(xor2, yor2)[2] / len(yor2), .25)\n",
    "    return a and b and c\n",
    "\n",
    "def sqsplit_test2():\n",
    "    x = np.array(range(1000)).reshape(-1,1)\n",
    "    y = np.hstack([np.ones(500),-1*np.ones(500)]).T\n",
    "    _, cut, _ = sqsplit(x, y)\n",
    "    return cut <= 500 or cut >= 499\n",
    "\n",
    "def sqsplit_test3():\n",
    "    fid, cut, loss = sqsplit(xor5,yor5)\n",
    "    # cut should be 0.5 but 0 is also accepted\n",
    "    return fid == 0 and (cut >= 0 or cut <= 1) and np.isclose(loss / len(yor5), 2/3)\n",
    "\n",
    "runtest(sqsplit_test1,'sqsplit_test1')\n",
    "runtest(sqsplit_test2,'sqsplit_test2')\n",
    "runtest(sqsplit_test3,'sqsplit_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cbffe6477a05dd8d9db574f0d94edcfa",
     "grade": true,
     "grade_id": "cell-sqsplit_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0b0da843388d7a9ddf59c80d408d3859",
     "grade": true,
     "grade_id": "cell-sqsplit_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86d84abb3781b6b263676a20c5ed6418",
     "grade": true,
     "grade_id": "cell-sqsplit_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs distance_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d358dfc0f25657b18024284428aec959",
     "grade": false,
     "grade_id": "cell-985b43cf6a0b1e16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Three: Implement `cart` [Graded]\n",
    "\n",
    "In this section, you will implement the function **`cart`**, which returns a regression tree based on the minimum squared loss splitting rule. You should use the function `sqsplit` to make your splits.\n",
    "\n",
    "**Implementation Notes:**\n",
    "We've provided a tree structure in the form of `TreeNode` for you that can be used for both leaves and nodes. To represent the leaves, you would set all fields except `prediction` to `None`.\n",
    "\n",
    "Non-leaf nodes will have non-`None` fields for all except `prediction`:\n",
    "1. `left`: node describing left subtree\n",
    "2. `right`: node describing right subtree\n",
    "3. `feature`: index of feature to cut (0-indexed as returned by `sqsplit`)\n",
    "4. `cut`: cutoff value $t$ ($\\leq t$: left and $> t$: right)\n",
    "5. `prediction`: `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0903afbf0f71752d3ae3caa2a24e8360",
     "grade": false,
     "grade_id": "cell-919899ceb491184f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"\n",
    "    Tree class.\n",
    "    \n",
    "    (You don't _need_ to add any methods or fields here but feel\n",
    "    free to if you like. The tests will only reference the fields\n",
    "    defined in the constructor below, so be sure to set these\n",
    "    correctly.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, feature, cut, prediction):\n",
    "        # Check that all or no arguments are None\n",
    "        node_or_leaf_args = [left, right, feature, cut]\n",
    "        assert all([arg == None for arg in node_or_leaf_args]) or all([arg != None for arg in node_or_leaf_args])\n",
    "        \n",
    "        # Check that all None <==> leaf <==> prediction not None\n",
    "        # Check that all non-None <==> non-leaf <==> prediction is None\n",
    "        if all([arg == None for arg in node_or_leaf_args]):\n",
    "            assert prediction is not None\n",
    "        if all([arg != None for arg in node_or_leaf_args]):\n",
    "            assert prediction is None\n",
    "        \n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.feature = feature \n",
    "        self.cut = cut\n",
    "        self.prediction = prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7eac0b2166f8fddde8c51b4ab8470e47",
     "grade": false,
     "grade_id": "cell-5b554dfec9394ba9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cell contains some examples of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a tree that predicts everything as zero ==> prediction 0\n",
    "# In this case, it has no left or right children (it is a leaf node) ==> left = None, right = None, feature = None, cut = None\n",
    "root = TreeNode(None, None, None, None, 0)\n",
    "\n",
    "\n",
    "# The following that a tree with depth 2 or a tree with one split \n",
    "\n",
    "# The tree will return a prediction of 1 if an example falls under the left subtree\n",
    "# Otherwise it will return a prediction of 2\n",
    "# To start, first create two leaf node\n",
    "left_leaf = TreeNode(None, None, None, None, 1)\n",
    "right_leaf = TreeNode(None, None, None, None, 2)\n",
    "\n",
    "# Now create the parent or the root\n",
    "# Suppose we split at feature 0 and cut of 1 and the prediction is None\n",
    "root2 = TreeNode(left_leaf, right_leaf, 0, 1, None)\n",
    "\n",
    "# Now root2 is the tree we desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9c2c5f1f300c14fa009977cd29781d7c",
     "grade": false,
     "grade_id": "cell-d6c63e37fd6c395f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the function `cart` using **recursion** (you call `cart` on the left and right subtrees inside the `cart` function). Recall the pseudocode for the CART algorithm.\n",
    "\n",
    "**NOTE:** In this implementation, you will be using **`np.mean`** for `prediction` argument. To check that floating point values in `xTr` are the same or not, you can use `np.isclose(xTr, xTr[0])`, which returns a list of `True` and `False` based on how different the rows of `xTr` are from the vector `xTr[0]`.\n",
    "\n",
    "<center><img src=\"cart-id3_pseudocode.png\" width=\"75%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de3750e532687607733d83dcc9a0a2dc",
     "grade": false,
     "grade_id": "cell-cart",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cart(xTr, yTr):\n",
    "    \"\"\"\n",
    "    Builds a CART tree.\n",
    "    \n",
    "    Input:\n",
    "        xTr:      n x d matrix of data\n",
    "        yTr:      n-dimensional vector\n",
    "\n",
    "    Output:\n",
    "        tree: root of decision tree\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    node = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc022deb75c9f2be5071b44c001e6d82",
     "grade": false,
     "grade_id": "cell-cart_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The tests below check that your implementation of cart  returns the correct predicted values for a sample dataset\n",
    "\n",
    "#test case 1\n",
    "def cart_test1():\n",
    "    t=cart(xor4,yor4)\n",
    "    return DFSxor(t)\n",
    "\n",
    "#test case 2\n",
    "def cart_test2():\n",
    "    y = np.random.rand(16);\n",
    "    t = cart(xor4,y);\n",
    "    yTe = DFSpreds(t)[:];\n",
    "    # Check that every label appears exactly once in the tree\n",
    "    y.sort()\n",
    "    yTe.sort()\n",
    "    return np.all(np.isclose(y, yTe))\n",
    "\n",
    "#test case 3\n",
    "def cart_test3():\n",
    "    xRep = np.concatenate([xor2, xor2])\n",
    "    yRep = np.concatenate([yor2, 1-yor2])\n",
    "    t = cart(xRep, yRep)\n",
    "    return DFSxorUnsplittable(t)\n",
    "\n",
    "#test case 4\n",
    "def cart_test4():\n",
    "    X = np.ones((5, 2)) # Create a dataset with identical examples\n",
    "    y = np.ones(5)\n",
    "    \n",
    "    # On this dataset, your cart algorithm should return a single leaf\n",
    "    # node with prediction equal to 1\n",
    "    t = cart(X, y)\n",
    "    \n",
    "    # t has no children\n",
    "    children_check = (t.left is None) and (t.right is None) \n",
    "    \n",
    "    # Make sure t does not cut any feature and at any value\n",
    "    feature_check = (t.feature is None) and (t.cut is None)\n",
    "    \n",
    "    # Check t's prediction\n",
    "    prediction_check = np.isclose(t.prediction, 1)\n",
    "    return children_check and feature_check and prediction_check\n",
    "\n",
    "#test case 5\n",
    "def cart_test5():\n",
    "    X = np.arange(4).reshape(-1, 1)\n",
    "    y = np.array([0, 0, 1, 1])\n",
    "\n",
    "    t = cart(X, y) # your cart algorithm should generate one split\n",
    "    \n",
    "    # check whether you set t.feature and t.cut to something\n",
    "    return t.feature is not None and t.cut is not None\n",
    "\n",
    "\n",
    "\n",
    "runtest(cart_test1,'cart_test1')\n",
    "runtest(cart_test2,'cart_test2')\n",
    "runtest(cart_test3,'cart_test3')\n",
    "runtest(cart_test4,'cart_test4')\n",
    "runtest(cart_test5,'cart_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b47e3031bb2beb61cae1c347d53b283",
     "grade": true,
     "grade_id": "cell-cart_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e322a7a1bab0a83d65ad27517c275cab",
     "grade": true,
     "grade_id": "cell-cart_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d6bf0051fdf9a7296e893eb66c16b96",
     "grade": true,
     "grade_id": "cell-cart_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07b92c1bbd6474aa04712ddb4dbcc0ca",
     "grade": true,
     "grade_id": "cell-cart_test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e3a55c57681ae193e2771512f553e528",
     "grade": true,
     "grade_id": "cell-cart_test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs cart_test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "faf306bddae40ade7985dcf6439b861c",
     "grade": false,
     "grade_id": "cell-91bac7e5c5968bbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Four: Implement `evaltree` [Graded]\n",
    "\n",
    "Implement the function **`evaltree`**, which evaluates a decision tree on a given test data set. You essentially need to traverse the tree until you end up in a leaf, where you return the `prediction` value of the leaf. Like the `cart` function, you can call `evaltree` on the left subtree and right subtree on testing points that fall in the corresponding subtrees.\n",
    "\n",
    "Here's some inspiration:\n",
    "1. If the `tree` is a leaf, i.e. the left and right subtrees are `None`, return `tree.prediction` for all `m` testing points.\n",
    "2. If the `tree` is non-leaf, using `tree.feature` and `tree.cut` find testing points with the feature value less than/equal to the threshold and greater than. Now, you can call `evaltree` on `tree.left` and the left set of testing points to obtain the left set's predictions. Then obtain the predictions for the right set, and return all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54f3f523c3529265fb0ce24f6fea9361",
     "grade": false,
     "grade_id": "cell-evaltree",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def evaltree(tree, xTe):\n",
    "    \"\"\"\n",
    "    Evaluates testing points in xTe using decision tree root.\n",
    "    \n",
    "    Input:\n",
    "        tree: TreeNode decision tree\n",
    "        xTe:  m x d matrix of data points\n",
    "    \n",
    "    Output:\n",
    "        pred: m-dimensional vector of predictions\n",
    "    \"\"\"\n",
    "    m, d = xTe.shape\n",
    "    preds = np.zeros(m)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59551843c4d5ab8b8c0924087bb9202c",
     "grade": false,
     "grade_id": "cell-evaltree_selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of evaltree returns the correct predictions for two sample trees\n",
    "\n",
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)\n",
    "\n",
    "#test case 1\n",
    "def evaltree_test1():\n",
    "    t = cart(xor4,yor4)\n",
    "    xor4te = xor4 + (np.sign(xor4 - .5) * .1)\n",
    "    inds = np.arange(16)\n",
    "    np.random.shuffle(inds)\n",
    "    # Check that shuffling and expanding the data doesn't affect the predictions\n",
    "    return np.all(np.isclose(evaltree(t, xor4te[inds,:]), yor4[inds]))\n",
    "\n",
    "#test case 2\n",
    "def evaltree_test2():\n",
    "    a = TreeNode(None, None, None, None, 1)\n",
    "    b = TreeNode(None, None, None, None, -1)\n",
    "    c = TreeNode(None, None, None, None, 0)\n",
    "    d = TreeNode(None, None, None, None, -1)\n",
    "    e = TreeNode(None, None, None, None, -1)\n",
    "    x = TreeNode(a, b, 0, 10, None)\n",
    "    y = TreeNode(x, c, 0, 20, None)\n",
    "    z = TreeNode(d, e, 0, 40, None)\n",
    "    t = TreeNode(y, z, 0, 30, None)\n",
    "    # Check that the custom tree evaluates correctly\n",
    "    return np.all(np.isclose(\n",
    "            evaltree(t, np.array([[45, 35, 25, 15, 5]]).T),\n",
    "            np.array([-1, -1, 0, -1, 1])))\n",
    "\n",
    "runtest(evaltree_test1,'evaltree_test1')\n",
    "runtest(evaltree_test2,'evaltree_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d772deb41878ca9f83b8c3661ac75b67",
     "grade": true,
     "grade_id": "cell-evaltree_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "68d89de2748bcdc384eb209588ffb84d",
     "grade": true,
     "grade_id": "cell-evaltree_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 Point\n",
    "# runs evaltree_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dfac59f41d855c0f8b4e1e7bf394c7d",
     "grade": false,
     "grade_id": "cell-030c38de62d6adc6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Training and Testing a Classification Tree on the ION Dataset</h3>\n",
    "\n",
    "<p> The following code create a classification tree on the ION dataset and then apply the learned tree to an unknown dataset. If you implement everything correctly, you should get a training RSME that is close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "root = cart(xTrIon, yTrIon)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((evaltree(root,xTrIon) - yTrIon)**2)\n",
    "te_err   = np.mean((evaltree(root,xTeIon) - yTeIon)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "efc7b56e0189a0d123d3ac40b1011afa",
     "grade": false,
     "grade_id": "cell-534ce4bb724b2f26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3> Visualize Your Tree</h3>\n",
    "\n",
    "<p>The following code defines a function <code>visclassifier()</code>, which plots the decision boundary of a classifier in 2 dimensions. Execute the following code to see what the decision boundary of your tree looks like on the ion data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "694a973d8d7c2709f769a08c64cf8f0f",
     "grade": false,
     "grade_id": "cell-8059df3cc7cdb39d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def visclassifier(fun,xTr,yTr,w=None,b=0):\n",
    "    \"\"\"\n",
    "    visualize decision boundary\n",
    "    Define the symbols and colors we'll use in the plots later\n",
    "    \"\"\"\n",
    "\n",
    "    yTr = np.array(yTr).flatten()\n",
    "    \n",
    "\n",
    "    symbols = [\"ko\",\"kx\"]\n",
    "    marker_symbols = ['o', 'x']\n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    # get the unique values from labels array\n",
    "    classvals = np.unique(yTr)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(min(xTr[:, 0]), max(xTr[:, 0]),res)\n",
    "    yrange = np.linspace(min(xTr[:, 1]), max(xTr[:, 1]),res)\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    \n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "\n",
    "    # creates x's and o's for training set\n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTr[yTr == c,0],\n",
    "            xTr[yTr == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    \n",
    "    if w is not None:\n",
    "        w = np.array(w).flatten()\n",
    "        alpha = -1 * b / (w ** 2).sum()\n",
    "        plt.quiver(w[0] * alpha, w[1] * alpha,\n",
    "            w[0], w[1], linewidth=2, color=[0,1,0])\n",
    "\n",
    "    plt.axis('tight')\n",
    "    # shows figure and blocks\n",
    "    plt.show()\n",
    "\n",
    "tree=cart(xTrSpiral,yTrSpiral) # compute tree on training data \n",
    "visclassifier(lambda X:evaltree(tree,X), xTrSpiral, yTrSpiral)\n",
    "print(\"Training error: %.4f\" % np.mean(np.sign(evaltree(tree,xTrSpiral)) != yTrSpiral))\n",
    "print(\"Testing error:  %.4f\" % np.mean(np.sign(evaltree(tree,xTeSpiral)) != yTeSpiral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f1fe9d13867759f9403c562fe900853a",
     "grade": false,
     "grade_id": "cell-8ad51604f6c3e28c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def onclick_cart(event):\n",
    "    \"\"\"\n",
    "    Visualize cart, including new point\n",
    "    \"\"\"\n",
    "    global xTraining,labels\n",
    "    # create position vector for new point\n",
    "    pos=np.array([[event.xdata,event.ydata]]) \n",
    "    if event.key == 'shift': # add positive point\n",
    "        color='or'\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        color='ob'\n",
    "        label=-1    \n",
    "    xTraining = np.concatenate((xTraining,pos), axis = 0)\n",
    "    labels.append(label);\n",
    "    marker_symbols = ['o', 'x']\n",
    "    classvals = np.unique(labels)\n",
    "        \n",
    "    mycolors = [[0.5, 0.5, 1], [1, 0.5, 0.5]]\n",
    "    \n",
    "    # return 300 evenly spaced numbers over this interval\n",
    "    res=300\n",
    "    xrange = np.linspace(0, 1,res)\n",
    "    yrange = np.linspace(0, 1,res)\n",
    "\n",
    "    \n",
    "    # repeat this matrix 300 times for both axes\n",
    "    pixelX = repmat(xrange, res, 1)\n",
    "    pixelY = repmat(yrange, res, 1).T\n",
    "\n",
    "    xTe = np.array([pixelX.flatten(), pixelY.flatten()]).T\n",
    "\n",
    "    # get decision tree\n",
    "    tree=cart(xTraining,np.array(labels).flatten())\n",
    "    fun = lambda X:evaltree(tree,X)\n",
    "    # test all of these points on the grid\n",
    "    testpreds = fun(xTe)\n",
    "    \n",
    "    # reshape it back together to make our grid\n",
    "    Z = testpreds.reshape(res, res)\n",
    "    # Z[0,0] = 1 # optional: scale the colors correctly\n",
    "    \n",
    "    plt.cla()    \n",
    "    plt.xlim((0,1))\n",
    "    plt.ylim((0,1))\n",
    "    # fill in the contours for these predictions\n",
    "    plt.contourf(pixelX, pixelY, np.sign(Z), colors=mycolors)\n",
    "    \n",
    "    for idx, c in enumerate(classvals):\n",
    "        plt.scatter(xTraining[labels == c,0],\n",
    "            xTraining[labels == c,1],\n",
    "            marker=marker_symbols[idx],\n",
    "            color='k'\n",
    "            )\n",
    "    plt.show()\n",
    "\n",
    "%matplotlib notebook\n",
    "xTraining= np.array([[5,6]])\n",
    "labels = [1]\n",
    "fig = plt.figure()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick_cart)\n",
    "plt.title('Click to add positive points and use shift-click to add negative points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "\n",
    "Scikit-learn also provides an implementation of [Regression Trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) (and [Decision Tree Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). The usage is pretty straight-forward: define the regression tree with the impurity function (and other settings), fit to the training set, and evaluate on any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "t0 = time.time()\n",
    "tree = DecisionTreeRegressor(\n",
    "    criterion='mse', # Impurity function = Mean Squared Error (squared loss)\n",
    "    splitter='best', # Take the best split\n",
    "    max_depth=None, # Expand the tree to the maximum depth possible\n",
    ")\n",
    "tree.fit(xTrSpiral, yTrSpiral)\n",
    "t1 = time.time()\n",
    "\n",
    "tr_err   = np.mean((tree.predict(xTrSpiral) - yTrSpiral)**2)\n",
    "te_err   = np.mean((tree.predict(xTeSpiral) - yTeSpiral)**2)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds\" % (t1-t0))\n",
    "print(\"Training RMSE : %.2f\" % tr_err)\n",
    "print(\"Testing  RMSE : %.2f \\n\" % te_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also provides a tree plotting function, which is again quite simple to use. This is extremely useful while debugging a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "_ = plot_tree(tree, ax=ax, precision=2, feature_names=[f'$[\\mathbf{{x}}]_{i+1}$' for i in range(2)], filled=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
